{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KHvrpQuTSENa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import hashlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/creditcard.csv')\n",
        "\n",
        "# Split features and labels\n",
        "X = data.drop('Class', axis=1).values  # Features\n",
        "y = data['Class'].values              # Target (0 = Non-fraud, 1 = Fraud)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
        "\n",
        "# Use only non-fraudulent samples for training\n",
        "X_normal = X[y == 0]\n",
        "X_anomaly = X[y == 1]\n",
        "\n",
        "X_anomaly.shape"
      ],
      "metadata": {
        "id": "u82CgeFjztoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e384e94a-d4d5-4fac-bec5-0932f1c452c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(492, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "batch_size = 4096\n",
        "X_train, X_test_normal = train_test_split(X_normal, test_size=0.01, random_state=42)\n",
        "X_normal_tensor = torch.tensor(X_normal.astype(np.float32), dtype=torch.float32)\n",
        "train_loader = torch.utils.data.DataLoader(X_normal_tensor, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Y6T3ydDdei3-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# import numpy as np\n",
        "# import torch.nn.utils.parametrizations as param\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# # PyTorch Generator Model\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, latent_dim, output_dim):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.fc1 = nn.Linear(latent_dim, 128)\n",
        "#         self.fc2 = nn.Linear(128, 256)\n",
        "#         self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "#         # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, z):\n",
        "#         z = F.relu(self.fc1(z))\n",
        "#         z = F.relu(self.fc2(z))\n",
        "#         output = self.fc3(z)  # Last layer (no activation)\n",
        "#         return output\n",
        "\n",
        "# # PyTorch Encoder Model\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, input_dim, latent_dim):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3 = nn.Linear(128, latent_dim)\n",
        "\n",
        "#         # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.leaky_relu(self.fc1(x),negative_slope=0.2)\n",
        "#         x = F.leaky_relu(self.fc2(x),negative_slope=0.2)\n",
        "#         x = self.fc3(x)  # Last layer (latent space) has no activation\n",
        "#         return x\n",
        "\n",
        "# # Define Discriminator Dxz\n",
        "# class DiscriminatorXZ(nn.Module):\n",
        "#     def __init__(self, x_dim, z_dim, do_spectral_norm=False):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             x_dim (int): Dimensionality of the x input.\n",
        "#             z_dim (int): Dimensionality of the z input.\n",
        "#             do_spectral_norm (bool): If True, apply spectral normalization to linear layers.\n",
        "#         \"\"\"\n",
        "#         super(DiscriminatorXZ, self).__init__()\n",
        "\n",
        "#         # Helper: apply spectral normalization if desired\n",
        "#         sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda layer: layer\n",
        "\n",
        "#         # D(x) branch: dense layer -> batch norm -> leaky ReLU\n",
        "#         self.x_fc1 = sn(nn.Linear(x_dim, 128))\n",
        "#         self.x_bn1 = nn.BatchNorm1d(128)\n",
        "\n",
        "#         # D(z) branch: dense layer -> leaky ReLU -> dropout\n",
        "#         self.z_fc1 = sn(nn.Linear(z_dim, 128))\n",
        "#         self.dropout = nn.Dropout(0.5)  # dropout rate 0.5\n",
        "\n",
        "#         # Combined branch (D(x,z)): after concatenation of x and z branches\n",
        "#         self.y_fc1 = sn(nn.Linear(128 + 128, 256))  # concatenated size = 256\n",
        "#         self.y_fc2 = sn(nn.Linear(256, 1))  # output logits\n",
        "\n",
        "#         # Xavier (Glorot) initialization for all linear layers\n",
        "#         nn.init.xavier_uniform_(self.x_fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.z_fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.y_fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.y_fc2.weight)\n",
        "\n",
        "#     def forward(self, x, z):\n",
        "#         # D(x) branch:\n",
        "#         x_out = self.x_fc1(x)\n",
        "#         x_out = self.x_bn1(x_out)\n",
        "#         x_out = F.leaky_relu(x_out,negative_slope=0.2)\n",
        "\n",
        "#         # D(z) branch:\n",
        "#         z_out = self.z_fc1(z)\n",
        "#         z_out = F.leaky_relu(z_out,negative_slope=0.2)\n",
        "#         z_out = self.dropout(z_out)  # dropout is active only in training mode\n",
        "\n",
        "#         # Concatenate the branches along the feature dimension\n",
        "#         y = torch.cat([x_out, z_out], dim=1)\n",
        "\n",
        "#         # Combined branch:\n",
        "#         y = self.y_fc1(y)\n",
        "#         y = F.leaky_relu(y,negative_slope=0.2)\n",
        "#         y = self.dropout(y)\n",
        "\n",
        "#         intermediate_layer = y  # For feature matching\n",
        "\n",
        "#         # Final logits layer (no activation)\n",
        "#         logits = self.y_fc2(y)\n",
        "\n",
        "#         return logits, intermediate_layer\n",
        "\n",
        "\n",
        "# # Define Discriminator Dxx\n",
        "# class DiscriminatorXX(nn.Module):\n",
        "#     def __init__(self, input_dim, do_spectral_norm=False):\n",
        "#         super(DiscriminatorXX, self).__init__()\n",
        "\n",
        "#         # Apply spectral normalization if enabled\n",
        "#         spectral_layer = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "#         # Fully connected layers with Spectral Normalization\n",
        "#         self.fc1 = spectral_layer(nn.Linear(input_dim * 2, 256))\n",
        "#         self.fc2 = spectral_layer(nn.Linear(256, 128))\n",
        "#         self.fc3 = spectral_layer(nn.Linear(128, 1))  # Final output layer\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.2)  # Dropout layer\n",
        "\n",
        "#         # Xavier Initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, x, rec_x):\n",
        "#         # Concatenate x and rec_x\n",
        "#         net = torch.cat([x, rec_x], dim=1)\n",
        "\n",
        "#         # Layer 1\n",
        "#         net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net) if self.training else net  # Dropout only during training\n",
        "\n",
        "#         # Layer 2\n",
        "#         net = F.leaky_relu(self.fc2(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net) if self.training else net  # Dropout only during training\n",
        "#         intermediate_layer = net\n",
        "\n",
        "#         # # Layer 3 (Logits)\n",
        "#         logits = self.fc3(net)  # No activation in final layer\n",
        "\n",
        "#         return logits, intermediate_layer\n",
        "\n",
        "# # Define Discriminator Dzz\n",
        "# class DiscriminatorZZ(nn.Module):\n",
        "#     def __init__(self, latent_dim, do_spectral_norm=False):\n",
        "#         super(DiscriminatorZZ, self).__init__()\n",
        "\n",
        "#         # If spectral normalization is desired, wrap the linear layers with it.\n",
        "#         sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "#         # First layer: input dimension is latent_dim * 2 due to concatenation of z and rec_z.\n",
        "#         self.fc1 = sn(nn.Linear(latent_dim * 2, 64))\n",
        "#         # Second layer.\n",
        "#         self.fc2 = sn(nn.Linear(64, 32))\n",
        "#         # Third (output) layer: produces logits.\n",
        "#         self.fc3 = sn(nn.Linear(32, 1))\n",
        "#         # Dropout layer with rate 0.2.\n",
        "#         self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "#         # Xavier initialization (Glorot Uniform)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, z, rec_z):\n",
        "#         # Concatenate along the feature dimension.\n",
        "#         net = torch.cat([z, rec_z], dim=1)\n",
        "\n",
        "#         # Layer 1: Dense -> Leaky ReLU -> Dropout.\n",
        "#         net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net)  # Dropout is active only in training mode.\n",
        "\n",
        "#         # Layer 2: Dense -> Leaky ReLU -> Dropout.\n",
        "#         net = F.leaky_relu(self.fc2(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net)\n",
        "\n",
        "#         # Save intermediate layer for feature matching.\n",
        "#         intermediate_layer = net\n",
        "\n",
        "#         # Layer 3: Dense to produce logits (no activation).\n",
        "#         logits = self.fc3(net)\n",
        "#         return logits, intermediate_layer\n",
        "\n"
      ],
      "metadata": {
        "id": "1ov8dIw95bJW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torch.nn.utils.parametrizations as param\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Generator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "\n",
        "        # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.relu(self.fc1(z))\n",
        "        output = self.fc2(z)  # Last layer (no activation)\n",
        "        return output\n",
        "\n",
        "# PyTorch Encoder Model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, latent_dim)\n",
        "\n",
        "        # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # Last layer (latent space) has no activation\n",
        "        return x\n",
        "\n",
        "# Define Discriminator Dxz\n",
        "class DiscriminatorXZ(nn.Module):\n",
        "    def __init__(self, x_dim, z_dim, do_spectral_norm=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_dim (int): Dimensionality of the x input.\n",
        "            z_dim (int): Dimensionality of the z input.\n",
        "            do_spectral_norm (bool): If True, apply spectral normalization to linear layers.\n",
        "        \"\"\"\n",
        "        super(DiscriminatorXZ, self).__init__()\n",
        "\n",
        "        # Helper: apply spectral normalization if desired\n",
        "        sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda layer: layer\n",
        "\n",
        "        # D(x) branch: dense layer -> batch norm -> leaky ReLU\n",
        "        self.x_fc1 = sn(nn.Linear(x_dim, 64))\n",
        "\n",
        "        # D(z) branch: dense layer -> leaky ReLU -> dropout\n",
        "        self.z_fc1 = sn(nn.Linear(z_dim, 64))\n",
        "\n",
        "        # Combined branch (D(x,z)): after concatenation of x and z branches\n",
        "        self.y_fc1 = sn(nn.Linear(64 + 64, 64))  # concatenated size = 256\n",
        "        self.y_fc2 = sn(nn.Linear(64, 1))  # output logits\n",
        "\n",
        "        # Xavier (Glorot) initialization for all linear layers\n",
        "        nn.init.xavier_uniform_(self.x_fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.z_fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.y_fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.y_fc2.weight)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        # D(x) branch:\n",
        "        x_out = self.x_fc1(x)\n",
        "\n",
        "        # D(z) branch:\n",
        "        z_out = self.z_fc1(z)\n",
        "\n",
        "        # Concatenate the branches along the feature dimension\n",
        "        y = torch.cat([x_out, z_out], dim=1)\n",
        "\n",
        "        # Combined branch:\n",
        "        y = self.y_fc1(y)\n",
        "        y = F.leaky_relu(y,negative_slope=0.2)\n",
        "\n",
        "        intermediate_layer = y  # For feature matching\n",
        "\n",
        "        # Final logits layer (no activation)\n",
        "        logits = self.y_fc2(y)\n",
        "\n",
        "        return logits, intermediate_layer\n",
        "\n",
        "\n",
        "# Define Discriminator Dxx\n",
        "class DiscriminatorXX(nn.Module):\n",
        "    def __init__(self, input_dim, do_spectral_norm=False):\n",
        "        super(DiscriminatorXX, self).__init__()\n",
        "\n",
        "        # Apply spectral normalization if enabled\n",
        "        spectral_layer = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "        # Fully connected layers with Spectral Normalization\n",
        "        self.fc1 = spectral_layer(nn.Linear(input_dim * 2, 64))\n",
        "        self.fc2 = spectral_layer(nn.Linear(64, 1))\n",
        "\n",
        "        # Xavier Initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x, rec_x):\n",
        "        # Concatenate x and rec_x\n",
        "        net = torch.cat([x, rec_x], dim=1)\n",
        "\n",
        "        # Layer 1\n",
        "        net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "        intermediate_layer = net\n",
        "\n",
        "        # # Layer 3 (Logits)\n",
        "        logits = self.fc2(net)  # No activation in final layer\n",
        "\n",
        "        return logits, intermediate_layer\n",
        "\n",
        "# Define Discriminator Dzz\n",
        "class DiscriminatorZZ(nn.Module):\n",
        "    def __init__(self, latent_dim, do_spectral_norm=False):\n",
        "        super(DiscriminatorZZ, self).__init__()\n",
        "\n",
        "        # If spectral normalization is desired, wrap the linear layers with it.\n",
        "        sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "        # First layer: input dimension is latent_dim * 2 due to concatenation of z and rec_z.\n",
        "        self.fc1 = sn(nn.Linear(latent_dim * 2, 64))\n",
        "        # Second layer.\n",
        "        self.fc2 = sn(nn.Linear(64, 1))\n",
        "        # Third (output) layer: produces logits.\n",
        "\n",
        "\n",
        "        # Xavier initialization (Glorot Uniform)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, z, rec_z):\n",
        "        # Concatenate along the feature dimension.\n",
        "        net = torch.cat([z, rec_z], dim=1)\n",
        "\n",
        "        # Layer 1: Dense -> Leaky ReLU -> Dropout.\n",
        "        net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "        intermediate_layer = net  # Dropout is active only in training mode.\n",
        "\n",
        "\n",
        "        # Layer 3: Dense to produce logits (no activation).\n",
        "        logits = self.fc2(net)\n",
        "        return logits, intermediate_layer\n",
        "\n"
      ],
      "metadata": {
        "id": "VkMugaE_1R3n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# import numpy as np\n",
        "# import torch.nn.utils.parametrizations as param\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# # PyTorch Generator Model\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, latent_dim, output_dim):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.fc1 = nn.Linear(latent_dim, 128)\n",
        "#         self.fc2 = nn.Linear(128, 256)\n",
        "#         self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "#         # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, z):\n",
        "#         z = F.relu(self.fc1(z))\n",
        "#         z = F.relu(self.fc2(z))\n",
        "#         output = self.fc3(z)  # Last layer (no activation)\n",
        "#         return output\n",
        "\n",
        "# # PyTorch Encoder Model\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, input_dim, latent_dim):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3 = nn.Linear(128, latent_dim)\n",
        "\n",
        "#         # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)  # Last layer (latent space) has no activation\n",
        "#         return x\n",
        "\n",
        "# # Define Discriminator Dxz\n",
        "# class DiscriminatorXZ(nn.Module):\n",
        "#     def __init__(self, x_dim, z_dim, do_spectral_norm=False):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             x_dim (int): Dimensionality of the x input.\n",
        "#             z_dim (int): Dimensionality of the z input.\n",
        "#             do_spectral_norm (bool): If True, apply spectral normalization to linear layers.\n",
        "#         \"\"\"\n",
        "#         super(DiscriminatorXZ, self).__init__()\n",
        "\n",
        "#         # Helper: apply spectral normalization if desired\n",
        "#         sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda layer: layer\n",
        "\n",
        "#         # D(x) branch: dense layer -> batch norm -> leaky ReLU\n",
        "#         self.x_fc1 = sn(nn.Linear(x_dim, 128))\n",
        "#         self.x_bn1 = nn.BatchNorm1d(128)\n",
        "\n",
        "#         # D(z) branch: dense layer -> leaky ReLU -> dropout\n",
        "#         self.z_fc1 = sn(nn.Linear(z_dim, 128))\n",
        "#         self.dropout = nn.Dropout(0.5)  # dropout rate 0.5\n",
        "\n",
        "#         # Combined branch (D(x,z)): after concatenation of x and z branches\n",
        "#         self.y_fc1 = sn(nn.Linear(128 + 128, 256))  # concatenated size = 256\n",
        "#         self.y_fc2 = sn(nn.Linear(256, 1))  # output logits\n",
        "\n",
        "#         # Xavier (Glorot) initialization for all linear layers\n",
        "#         nn.init.xavier_uniform_(self.x_fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.z_fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.y_fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.y_fc2.weight)\n",
        "\n",
        "#     def forward(self, x, z):\n",
        "#         # D(x) branch:\n",
        "#         x_out = self.x_fc1(x)\n",
        "#         x_out = self.x_bn1(x_out)\n",
        "#         x_out = F.leaky_relu(x_out,negative_slope=0.2)\n",
        "\n",
        "#         # D(z) branch:\n",
        "#         z_out = self.z_fc1(z)\n",
        "#         z_out = F.leaky_relu(z_out,negative_slope=0.2)\n",
        "#         z_out = self.dropout(z_out)  # dropout is active only in training mode\n",
        "\n",
        "#         # Concatenate the branches along the feature dimension\n",
        "#         y = torch.cat([x_out, z_out], dim=1)\n",
        "\n",
        "#         # Combined branch:\n",
        "#         y = self.y_fc1(y)\n",
        "#         y = F.leaky_relu(y,negative_slope=0.2)\n",
        "#         y = self.dropout(y)\n",
        "\n",
        "#         intermediate_layer = y  # For feature matching\n",
        "\n",
        "#         # Final logits layer (no activation)\n",
        "#         logits = self.y_fc2(y)\n",
        "\n",
        "#         return logits, intermediate_layer\n",
        "\n",
        "\n",
        "# # Define Discriminator Dxx\n",
        "# class DiscriminatorXX(nn.Module):\n",
        "#     def __init__(self, input_dim, do_spectral_norm=False):\n",
        "#         super(DiscriminatorXX, self).__init__()\n",
        "\n",
        "#         # Apply spectral normalization if enabled\n",
        "#         spectral_layer = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "#         # Fully connected layers with Spectral Normalization\n",
        "#         self.fc1 = spectral_layer(nn.Linear(input_dim * 2, 256))\n",
        "#         self.fc2 = spectral_layer(nn.Linear(256, 128))\n",
        "#         self.fc3 = spectral_layer(nn.Linear(128, 1))  # Final output layer\n",
        "\n",
        "#         self.dropout = nn.Dropout(0.2)  # Dropout layer\n",
        "\n",
        "#         # Xavier Initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, x, rec_x):\n",
        "#         # Concatenate x and rec_x\n",
        "#         net = torch.cat([x, rec_x], dim=1)\n",
        "\n",
        "#         # Layer 1\n",
        "#         net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net) if self.training else net  # Dropout only during training\n",
        "\n",
        "#         # Layer 2\n",
        "#         net = F.leaky_relu(self.fc2(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net) if self.training else net  # Dropout only during training\n",
        "#         intermediate_layer = net\n",
        "\n",
        "#         # # Layer 3 (Logits)\n",
        "#         logits = self.fc3(net)  # No activation in final layer\n",
        "\n",
        "#         return logits, intermediate_layer\n",
        "\n",
        "# # Define Discriminator Dzz\n",
        "# class DiscriminatorZZ(nn.Module):\n",
        "#     def __init__(self, latent_dim, do_spectral_norm=False):\n",
        "#         super(DiscriminatorZZ, self).__init__()\n",
        "\n",
        "#         # If spectral normalization is desired, wrap the linear layers with it.\n",
        "#         sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "#         # First layer: input dimension is latent_dim * 2 due to concatenation of z and rec_z.\n",
        "#         self.fc1 = sn(nn.Linear(latent_dim * 2, 64))\n",
        "#         # Second layer.\n",
        "#         self.fc2 = sn(nn.Linear(64, 32))\n",
        "#         # Third (output) layer: produces logits.\n",
        "#         self.fc3 = sn(nn.Linear(32, 1))\n",
        "#         # Dropout layer with rate 0.2.\n",
        "#         self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "#         # Xavier initialization (Glorot Uniform)\n",
        "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
        "#         nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "#     def forward(self, z, rec_z):\n",
        "#         # Concatenate along the feature dimension.\n",
        "#         net = torch.cat([z, rec_z], dim=1)\n",
        "\n",
        "#         # Layer 1: Dense -> Leaky ReLU -> Dropout.\n",
        "#         net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net)  # Dropout is active only in training mode.\n",
        "\n",
        "#         # Layer 2: Dense -> Leaky ReLU -> Dropout.\n",
        "#         net = F.leaky_relu(self.fc2(net),negative_slope=0.2)\n",
        "#         net = self.dropout(net)\n",
        "\n",
        "#         # Save intermediate layer for feature matching.\n",
        "#         intermediate_layer = net\n",
        "\n",
        "#         # Layer 3: Dense to produce logits (no activation).\n",
        "#         logits = self.fc3(net)\n",
        "#         return logits, intermediate_layer"
      ],
      "metadata": {
        "id": "6dgcxFyrsp3p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "batch_size = 128\n",
        "X_train, X_test_normal = train_test_split(X_normal, test_size=0.01, random_state=42)\n",
        "X_normal_tensor = torch.tensor(X_normal.astype(np.float32), dtype=torch.float32)\n",
        "train_loader = torch.utils.data.DataLoader(X_normal_tensor, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "0QFT0Lzus_W8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "# Training Loop\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = X_normal.shape[1]\n",
        "learning_rate_gen = 1e-3   # Generator (Encoder)\n",
        "learning_rate_disc_xx = 2e-4  # Discriminator (lower than encoder)\n",
        "learning_rate_enc = 1e-3\n",
        "learning_rate_disc_xz = 1e-6\n",
        "learning_rate_disc_zz = 2e-4\n",
        "latent_dim = 16\n",
        "x_dim = input_dim\n",
        "num_epochs = 50\n",
        "log_interval = 100\n",
        "\n",
        "learning_rate_gen = 1e-3   # Generator (Encoder)\n",
        "learning_rate_disc_xx = 2e-4 # Discriminator (lower than encoder)\n",
        "learning_rate_enc = 1e-3\n",
        "learning_rate_disc_xz = 1e-6\n",
        "learning_rate_disc_zz = 1e-6\n",
        "\n",
        "# Loss function\n",
        "criterion_s = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "criterion_m = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "criterion_n = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "# Move models to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize networks (example constructors; adapt as needed)\n",
        "encoder = Encoder(x_dim, latent_dim).to(device)\n",
        "generator = Generator(latent_dim, x_dim).to(device)\n",
        "discriminator_xz = DiscriminatorXZ(x_dim, latent_dim,do_spectral_norm=True).to(device)\n",
        "discriminator_xx = DiscriminatorXX(x_dim,do_spectral_norm=True).to(device)\n",
        "discriminator_zz = DiscriminatorZZ(latent_dim,do_spectral_norm=False).to(device)\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "optimizer_D_xz = optim.Adam(discriminator_xz.parameters(), lr=learning_rate_disc_xz, betas=(0.5, 0.9))\n",
        "optimizer_D_xx = optim.Adam(discriminator_xx.parameters(), lr=learning_rate_disc_xx, betas=(0.5, 0.9))\n",
        "optimizer_D_zz = optim.Adam(discriminator_zz.parameters(), lr=learning_rate_disc_zz, betas=(0.5, 0.9))\n",
        "\n",
        "# Separate optimizers for generator and encoder\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_gen, betas=(0.5, 0.9))\n",
        "optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate_enc, betas=(0.5, 0.9))\n",
        "\n",
        "# Define a clipping value (adjust as needed)\n",
        "discriminator_update_interval=2\n",
        "# recon_criterion = torch.nn.L1Loss()\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    generator.train()\n",
        "    discriminator_xz.train()\n",
        "    discriminator_xx.train()\n",
        "    discriminator_zz.train()\n",
        "\n",
        "    # For logging losses per epoch (average per sample)\n",
        "    total_loss_D_xz = 0.0\n",
        "    total_loss_D_xx = 0.0\n",
        "    total_loss_D_zz = 0.0\n",
        "    total_loss_G = 0.0\n",
        "    total_loss_E = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, real_x in enumerate(train_loader):\n",
        "        # real_x = real_x.to(device)\n",
        "        real_x = real_x.type(torch.FloatTensor).to(device)\n",
        "        # real_x.requires_grad = True\n",
        "        batch_size = real_x.size(0)\n",
        "        current_batch_size = batch_size\n",
        "        n_batches += 1\n",
        "\n",
        "        # Define labels (adapted: real=0, fake=1)\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "        if i % discriminator_update_interval == 0:\n",
        "            # ============================\n",
        "            # 1. Update Discriminator_xz\n",
        "            # ============================\n",
        "            optimizer_D_xz.zero_grad()\n",
        "\n",
        "            # Real pairs: (x, encoder(x))\n",
        "            z_enc = encoder(real_x)\n",
        "            logits_real_xz, _ = discriminator_xz(real_x, z_enc)\n",
        "            loss_real_xz = criterion_m(logits_real_xz, real_labels)\n",
        "\n",
        "            # Fake pairs: (generator(z_noise), z_noise)\n",
        "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "            x_fake = generator(z_noise).detach()\n",
        "            logits_fake_xz, _ = discriminator_xz(x_fake, z_noise)\n",
        "            loss_fake_xz = criterion_m(logits_fake_xz, fake_labels)\n",
        "\n",
        "            loss_D_xz = (loss_real_xz + loss_fake_xz) #(loss_real_xz + loss_fake_xz) / 2\n",
        "            loss_D_xz.backward()\n",
        "            optimizer_D_xz.step()\n",
        "\n",
        "            # ============================\n",
        "            # 2. Update Discriminator_xx\n",
        "            # ============================\n",
        "            optimizer_D_xx.zero_grad()\n",
        "\n",
        "            # Real pairs: (x, x)\n",
        "            logits_real_xx, _ = discriminator_xx(real_x, real_x)\n",
        "            loss_real_xx = criterion_s(logits_real_xx, real_labels)\n",
        "\n",
        "            # Fake pairs: (x, generator(encoder(x)))\n",
        "            x_rec = generator(encoder(real_x)).detach()\n",
        "            logits_fake_xx, _ = discriminator_xx(real_x, x_rec)\n",
        "            loss_fake_xx = criterion_s(logits_fake_xx, fake_labels)\n",
        "\n",
        "            loss_D_xx = torch.mean(loss_real_xx + loss_fake_xx)  #(loss_real_xx + loss_fake_xx) / 2\n",
        "            loss_D_xx.backward()\n",
        "            optimizer_D_xx.step()\n",
        "\n",
        "            # ============================\n",
        "            # 3. Update Discriminator_zz\n",
        "            # ============================\n",
        "            optimizer_D_zz.zero_grad()\n",
        "\n",
        "            # Real pairs: (z, z) where z is sampled from the prior (noise)\n",
        "            z_prior = torch.randn(batch_size, latent_dim, device=device)\n",
        "            logits_real_zz, _ = discriminator_zz(z_prior, z_prior)\n",
        "            loss_real_zz = criterion_s(logits_real_zz, real_labels)\n",
        "\n",
        "            # Fake pairs: (z, encoder(generator(z)))\n",
        "            x_fake = generator(z_prior)\n",
        "            z_rec = encoder(x_fake).detach()\n",
        "            logits_fake_zz, _ = discriminator_zz(z_prior, z_rec)\n",
        "            loss_fake_zz = criterion_s(logits_fake_zz, fake_labels)\n",
        "\n",
        "            loss_D_zz = torch.mean(loss_real_zz + loss_fake_zz)#(loss_real_zz + loss_fake_zz) / 2\n",
        "            loss_D_zz.backward()\n",
        "            optimizer_D_zz.step()\n",
        "\n",
        "        #### GEN Code\n",
        "        # Assume the following tensors are already computed:\n",
        "        # l_generator: logits from some branch of the generator (used for adversarial loss)\n",
        "        # l_encoder: logits from some branch of the encoder\n",
        "        # x_logit_real, x_logit_fake: discriminator outputs (logits) for the x branch (real x and reconstructed x)\n",
        "        # z_logit_real, z_logit_fake: discriminator outputs (logits) for the z branch (real z and reconstructed z)\n",
        "        # allow_zz: Boolean flag indicating whether to include the z branch in cycle consistency loss\n",
        "        z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "        x_fake = generator(z_noise)\n",
        "        l_generator, _ = discriminator_xz(x_fake, z_noise)\n",
        "        # Adversarial loss for the generator: now we use real label = 0 (instead of 1)\n",
        "        gen_loss_xz = criterion_m(l_generator, torch.ones_like(l_generator))\n",
        "\n",
        "        real_x_clone = real_x.clone()\n",
        "\n",
        "        # Cycle consistency loss for the x branch:\n",
        "        # For real x (should be classified as real, i.e., 0)\n",
        "\n",
        "        x_real_dis, _ = discriminator_xx(real_x_clone, real_x_clone)\n",
        "\n",
        "        # # Fake pairs: (x, generator(encoder(x)))\n",
        "        x_real_gen = criterion_s(x_real_dis, torch.zeros_like(x_real_dis))\n",
        "\n",
        "        x_rec = generator(encoder(real_x_clone))\n",
        "        x_fake_dis, _ = discriminator_xx(real_x_clone, x_rec)\n",
        "\n",
        "        # For fake (reconstructed) x (should be classified as fake, i.e., 1)\n",
        "        x_fake_gen = criterion_s(x_fake_dis, torch.ones_like(x_fake_dis))\n",
        "\n",
        "        cost_x = torch.mean(x_real_gen + x_fake_gen).clone()\n",
        "\n",
        "        # Cycle consistency loss for the z branch:\n",
        "\n",
        "        # # Real pairs: (z, z) where z is sampled from the prior (noise)\n",
        "        z_prior = torch.randn(batch_size, latent_dim, device=device)\n",
        "        z_real_dis, _ = discriminator_zz(z_prior, z_prior)\n",
        "\n",
        "        # # Fake pairs: (z, encoder(generator(z)))\n",
        "        x_fake = generator(z_prior)\n",
        "        z_rec = encoder(x_fake)\n",
        "        z_fake_dis, _ = discriminator_zz(z_prior, z_rec)\n",
        "\n",
        "\n",
        "        z_real_gen = criterion_s(z_real_dis, torch.ones_like(z_real_dis))\n",
        "        z_fake_gen = criterion_s(z_fake_dis, torch.zeros_like(z_fake_dis))\n",
        "        cost_z = torch.mean(z_real_gen + z_fake_gen)\n",
        "\n",
        "        # # --- Cycle consistency loss (Reconstruction Loss) ---\n",
        "        # # Compute reconstruction x_rec = G(E(x)) and compare to real x.\n",
        "        # x_rec = generator(encoder(real_x))\n",
        "        # recon_loss = recon_criterion(x_rec, real_x)\n",
        "        # cycle_consistency_loss = recon_loss\n",
        "\n",
        "        # Total cycle-consistency loss: include z branch if allowed\n",
        "        cycle_consistency_loss = cost_x + cost_z\n",
        "        # cycle_consistency_loss = cost_x\n",
        "\n",
        "        # Final losses:\n",
        "        # loss_generator = gen_loss_xz + cycle_consistency_loss\n",
        "        # loss_generator.backward(retain_graph=True)\n",
        "        # optimizer_G.step()\n",
        "\n",
        "        # loss_encoder = enc_loss_xz + cycle_consistency_loss\n",
        "        # loss_encoder.backward()\n",
        "        # optimizer_E.step()\n",
        "\n",
        "        lambda_cycle = 5.0  # You can increase this value to enforce stronger cycle consistency.\n",
        "\n",
        "        # Final losses for the generator and encoder:\n",
        "        loss_generator = gen_loss_xz + lambda_cycle * cycle_consistency_loss\n",
        "\n",
        "        loss_generator.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # For the encoder, we use the loss computed from discriminator_xz.\n",
        "        l_encoder, _ = discriminator_xz(real_x_clone, encoder(real_x_clone))\n",
        "        # Adversarial loss for the generator: now we use real label = 0 (instead of 1)\n",
        "        enc_loss_xz = criterion_m(l_encoder, torch.zeros_like(l_encoder))\n",
        "\n",
        "        x_real_dis, _ = discriminator_xx(real_x_clone, real_x_clone)\n",
        "        x_real_gen = criterion_m(x_real_dis, torch.zeros_like(x_real_dis))\n",
        "\n",
        "        # # Fake pairs: (x, generator(encoder(x)))\n",
        "        x_rec = generator(encoder(real_x_clone))\n",
        "        x_fake_dis, _ = discriminator_xx(real_x_clone, x_rec)\n",
        "\n",
        "\n",
        "        # For fake (reconstructed) x (should be classified as fake, i.e., 1)\n",
        "        x_fake_gen = criterion_m(x_fake_dis, torch.ones_like(x_fake_dis))\n",
        "\n",
        "        cost_x = torch.mean(x_real_gen + x_fake_gen).clone()\n",
        "\n",
        "                 # # Real pairs: (z, z) where z is sampled from the prior (noise)\n",
        "        z_prior = torch.randn(batch_size, latent_dim, device=device)\n",
        "        z_real_dis, _ = discriminator_zz(z_prior, z_prior)\n",
        "\n",
        "        # # Fake pairs: (z, encoder(generator(z)))\n",
        "        x_fake = generator(z_prior)\n",
        "        z_rec = encoder(x_fake)\n",
        "        z_fake_dis, _ = discriminator_zz(z_prior, z_rec)\n",
        "\n",
        "\n",
        "        z_real_gen = criterion_s(z_real_dis, torch.zeros_like(z_real_dis))\n",
        "        z_fake_gen = criterion_s(z_fake_dis, torch.ones_like(z_fake_dis))\n",
        "        cost_z = torch.mean(z_real_gen + z_fake_gen)\n",
        "\n",
        "        cycle_consistency_loss = cost_x + cost_z\n",
        "        loss_encoder = (enc_loss_xz + lambda_cycle * (cycle_consistency_loss )).clone()\n",
        "        optimizer_E.zero_grad()\n",
        "        loss_encoder.backward()  # No retain_graph needed, last step\n",
        "        optimizer_E.step()\n",
        "\n",
        "\n",
        "\n",
        "        # Log per-iteration (average loss per sample)\n",
        "        total_loss_D_xz += loss_D_xz.item() / current_batch_size if i % discriminator_update_interval == 0 else 0\n",
        "        total_loss_D_xx += loss_D_xx.item() / current_batch_size if i % discriminator_update_interval == 0 else 0\n",
        "        total_loss_D_zz += loss_D_zz.item() / current_batch_size if i % discriminator_update_interval == 0 else 0\n",
        "        total_loss_G    += loss_generator.item() / current_batch_size\n",
        "        total_loss_E    += loss_encoder.item() / current_batch_size\n",
        "\n",
        "\n",
        "    # End of epoch: compute and log average losses for discriminators (if updated)\n",
        "    num_disc_updates = (n_batches // discriminator_update_interval) or 1\n",
        "    avg_loss_D_xz = total_loss_D_xz / num_disc_updates\n",
        "    avg_loss_D_xx = total_loss_D_xx / num_disc_updates\n",
        "    avg_loss_D_zz = total_loss_D_zz / num_disc_updates\n",
        "    avg_loss_G = total_loss_G / n_batches\n",
        "    avg_loss_E = total_loss_E / n_batches\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Summary:\")\n",
        "    print(f\"  Average Loss_D_xz: {avg_loss_D_xz:.4f}  Average Loss_D_xx: {avg_loss_D_xx:.4f} Average Loss_D_zz: {avg_loss_D_zz:.4f}\")\n",
        "    # print(f\"  Average Loss_D_xz: {avg_loss_D_xz:.4f}  Average Loss_D_xx: {avg_loss_D_xx:.4f}\")\n",
        "    print(f\"  Average Loss_G: {avg_loss_G:.4f}, Average Loss_E: {avg_loss_E:.4f}\")\n",
        "\n",
        "print(\"Training Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kH9NUBb_iK3e",
        "outputId": "cd504025-f4e6-428e-f05b-8545f10316a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Summary:\n",
            "  Average Loss_D_xz: 0.0185  Average Loss_D_xx: 1.3478 Average Loss_D_zz: 14.3191\n",
            "  Average Loss_G: 81.3315, Average Loss_E: 3.9918\n",
            "Epoch [2/50] Summary:\n",
            "  Average Loss_D_xz: 0.0111  Average Loss_D_xx: 1.0096 Average Loss_D_zz: 25.1001\n",
            "  Average Loss_G: 143.0645, Average Loss_E: 3.9720\n",
            "Epoch [3/50] Summary:\n",
            "  Average Loss_D_xz: 0.0121  Average Loss_D_xx: 3.7591 Average Loss_D_zz: 28.9025\n",
            "  Average Loss_G: 154.8237, Average Loss_E: 3.9148\n",
            "Epoch [4/50] Summary:\n",
            "  Average Loss_D_xz: 0.0119  Average Loss_D_xx: 2.1524 Average Loss_D_zz: 32.4360\n",
            "  Average Loss_G: 188.9471, Average Loss_E: 4.0066\n",
            "Epoch [5/50] Summary:\n",
            "  Average Loss_D_xz: 0.0092  Average Loss_D_xx: 4.1946 Average Loss_D_zz: 46.8867\n",
            "  Average Loss_G: 250.5547, Average Loss_E: 3.9018\n",
            "Epoch [6/50] Summary:\n",
            "  Average Loss_D_xz: 0.0102  Average Loss_D_xx: 5.2896 Average Loss_D_zz: 61.5465\n",
            "  Average Loss_G: 318.1255, Average Loss_E: 3.8616\n",
            "Epoch [7/50] Summary:\n",
            "  Average Loss_D_xz: 0.0115  Average Loss_D_xx: 7.0924 Average Loss_D_zz: 53.1591\n",
            "  Average Loss_G: 282.7209, Average Loss_E: 3.8879\n",
            "Epoch [8/50] Summary:\n",
            "  Average Loss_D_xz: 0.0131  Average Loss_D_xx: 3.6901 Average Loss_D_zz: 52.6169\n",
            "  Average Loss_G: 292.8079, Average Loss_E: 3.9777\n",
            "Epoch [9/50] Summary:\n",
            "  Average Loss_D_xz: 0.0118  Average Loss_D_xx: 14.8101 Average Loss_D_zz: 53.6474\n",
            "  Average Loss_G: 285.9325, Average Loss_E: 3.8785\n",
            "Epoch [10/50] Summary:\n",
            "  Average Loss_D_xz: 0.0079  Average Loss_D_xx: 2.9443 Average Loss_D_zz: 52.3852\n",
            "  Average Loss_G: 307.1831, Average Loss_E: 4.0847\n",
            "Epoch [11/50] Summary:\n",
            "  Average Loss_D_xz: 0.0080  Average Loss_D_xx: 2.1525 Average Loss_D_zz: 67.3565\n",
            "  Average Loss_G: 373.3702, Average Loss_E: 3.9997\n",
            "Epoch [12/50] Summary:\n",
            "  Average Loss_D_xz: 0.0070  Average Loss_D_xx: 0.3558 Average Loss_D_zz: 49.8371\n",
            "  Average Loss_G: 317.7487, Average Loss_E: 4.2564\n",
            "Epoch [13/50] Summary:\n",
            "  Average Loss_D_xz: 0.0080  Average Loss_D_xx: 22.7601 Average Loss_D_zz: 70.4139\n",
            "  Average Loss_G: 398.3924, Average Loss_E: 4.0931\n",
            "Epoch [14/50] Summary:\n",
            "  Average Loss_D_xz: 0.0088  Average Loss_D_xx: 5.3130 Average Loss_D_zz: 70.9057\n",
            "  Average Loss_G: 382.9145, Average Loss_E: 3.9303\n",
            "Epoch [15/50] Summary:\n",
            "  Average Loss_D_xz: 0.0082  Average Loss_D_xx: 1.3239 Average Loss_D_zz: 55.3170\n",
            "  Average Loss_G: 380.4775, Average Loss_E: 4.5279\n",
            "Epoch [16/50] Summary:\n",
            "  Average Loss_D_xz: 0.0066  Average Loss_D_xx: 91.3425 Average Loss_D_zz: 71.8243\n",
            "  Average Loss_G: 460.9414, Average Loss_E: 4.4748\n",
            "Epoch [17/50] Summary:\n",
            "  Average Loss_D_xz: 0.0070  Average Loss_D_xx: 15.0036 Average Loss_D_zz: 83.9671\n",
            "  Average Loss_G: 544.7652, Average Loss_E: 4.6754\n",
            "Epoch [18/50] Summary:\n",
            "  Average Loss_D_xz: 0.0066  Average Loss_D_xx: 28.2073 Average Loss_D_zz: 112.8359\n",
            "  Average Loss_G: 577.4829, Average Loss_E: 3.7704\n",
            "Epoch [19/50] Summary:\n",
            "  Average Loss_D_xz: 0.0069  Average Loss_D_xx: 14.1261 Average Loss_D_zz: 145.4454\n",
            "  Average Loss_G: 742.3933, Average Loss_E: 3.7957\n",
            "Epoch [20/50] Summary:\n",
            "  Average Loss_D_xz: 0.0079  Average Loss_D_xx: 36.4607 Average Loss_D_zz: 193.4990\n",
            "  Average Loss_G: 987.1365, Average Loss_E: 3.8256\n",
            "Epoch [21/50] Summary:\n",
            "  Average Loss_D_xz: 0.0060  Average Loss_D_xx: 62.7766 Average Loss_D_zz: 198.6409\n",
            "  Average Loss_G: 1008.1885, Average Loss_E: 3.7751\n",
            "Epoch [22/50] Summary:\n",
            "  Average Loss_D_xz: 0.0052  Average Loss_D_xx: 10.5218 Average Loss_D_zz: 163.5444\n",
            "  Average Loss_G: 847.3319, Average Loss_E: 3.9014\n",
            "Epoch [23/50] Summary:\n",
            "  Average Loss_D_xz: 0.0056  Average Loss_D_xx: 13.8473 Average Loss_D_zz: 156.2094\n",
            "  Average Loss_G: 815.7455, Average Loss_E: 3.9280\n",
            "Epoch [24/50] Summary:\n",
            "  Average Loss_D_xz: 0.0055  Average Loss_D_xx: 12.2099 Average Loss_D_zz: 161.0892\n",
            "  Average Loss_G: 835.4444, Average Loss_E: 3.8576\n",
            "Epoch [25/50] Summary:\n",
            "  Average Loss_D_xz: 0.0054  Average Loss_D_xx: 2.2827 Average Loss_D_zz: 165.7795\n",
            "  Average Loss_G: 869.7259, Average Loss_E: 3.9404\n",
            "Epoch [26/50] Summary:\n",
            "  Average Loss_D_xz: 0.0049  Average Loss_D_xx: 31.2453 Average Loss_D_zz: 216.7382\n",
            "  Average Loss_G: 1111.4874, Average Loss_E: 3.8070\n",
            "Epoch [27/50] Summary:\n",
            "  Average Loss_D_xz: 0.0047  Average Loss_D_xx: 1.1076 Average Loss_D_zz: 140.9092\n",
            "  Average Loss_G: 738.2148, Average Loss_E: 3.8693\n",
            "Epoch [28/50] Summary:\n",
            "  Average Loss_D_xz: 0.0052  Average Loss_D_xx: 0.1706 Average Loss_D_zz: 138.0137\n",
            "  Average Loss_G: 737.9162, Average Loss_E: 4.0048\n",
            "Epoch [29/50] Summary:\n",
            "  Average Loss_D_xz: 0.0053  Average Loss_D_xx: 30.5593 Average Loss_D_zz: 128.6864\n",
            "  Average Loss_G: 683.0462, Average Loss_E: 3.9226\n",
            "Epoch [30/50] Summary:\n",
            "  Average Loss_D_xz: 0.0048  Average Loss_D_xx: 0.7788 Average Loss_D_zz: 150.7864\n",
            "  Average Loss_G: 817.4479, Average Loss_E: 4.1835\n",
            "Epoch [31/50] Summary:\n",
            "  Average Loss_D_xz: 0.0045  Average Loss_D_xx: 4.8448 Average Loss_D_zz: 149.1932\n",
            "  Average Loss_G: 809.3290, Average Loss_E: 4.1715\n",
            "Epoch [32/50] Summary:\n",
            "  Average Loss_D_xz: 0.0050  Average Loss_D_xx: 10.4310 Average Loss_D_zz: 135.3927\n",
            "  Average Loss_G: 742.4770, Average Loss_E: 4.1815\n",
            "Epoch [33/50] Summary:\n",
            "  Average Loss_D_xz: 0.0049  Average Loss_D_xx: 91.5009 Average Loss_D_zz: 97.3138\n",
            "  Average Loss_G: 567.7328, Average Loss_E: 4.2327\n",
            "Epoch [34/50] Summary:\n",
            "  Average Loss_D_xz: 0.0048  Average Loss_D_xx: 3.2149 Average Loss_D_zz: 80.4255\n",
            "  Average Loss_G: 470.7573, Average Loss_E: 4.1952\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-56101788f3dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# For fake (reconstructed) x (should be classified as fake, i.e., 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mx_fake_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fake_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fake_dis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mcost_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_real_gen\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_fake_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         return F.binary_cross_entropy_with_logits(\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3626\u001b[0m         )\n\u001b[1;32m   3627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3628\u001b[0;31m     return torch.binary_cross_entropy_with_logits(\n\u001b[0m\u001b[1;32m   3629\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# --- Sampling ---\n",
        "num_samples_normal = 300\n",
        "num_samples_anomaly = 300\n",
        "\n",
        "# Sample from your normal and anomaly DataFrames (assume these are defined)\n",
        "X_test_normal_sampled = pd.DataFrame(X_normal).sample(n=num_samples_normal, random_state=42)\n",
        "X_anomaly_sampled = pd.DataFrame(X_anomaly).sample(n=num_samples_anomaly, random_state=42)\n",
        "\n",
        "# Create combined test dataset and labels\n",
        "X_test = pd.concat([X_test_normal_sampled, X_anomaly_sampled])\n",
        "y_test = np.concatenate([np.zeros(num_samples_normal, dtype=int),\n",
        "                         np.ones(num_samples_anomaly, dtype=int)])\n",
        "\n",
        "# --- Shuffling ---\n",
        "# Shuffle the rows of X_test and y_test together\n",
        "shuffled_indices = np.random.permutation(X_test.index)\n",
        "X_test_shuffled = X_test.loc[shuffled_indices]\n",
        "y_test_shuffled = y_test[np.argsort(shuffled_indices)]  # This works if shuffled_indices is sorted;\n",
        "# Alternatively, we can use:\n",
        "# y_test_shuffled = y_test[list(shuffled_indices)]\n",
        "\n",
        "# --- Conversion to Tensors ---\n",
        "# Convert the shuffled DataFrame and labels to torch tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_x_tensor = torch.tensor(X_test_shuffled.values, dtype=torch.float32).to(device)\n",
        "test_y_tensor = torch.tensor(y_test_shuffled, dtype=torch.long).to(device)\n",
        "\n",
        "# Create a TensorDataset and DataLoader\n",
        "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
        "batch_size = 1  # Adjust as needed\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check the shapes\n",
        "print(\"Test X tensor shape:\", test_x_tensor.shape)\n",
        "print(\"Test Y tensor shape:\", test_y_tensor.shape)\n"
      ],
      "metadata": {
        "id": "_REpzaxyF-Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "encoder.eval()\n",
        "generator.eval()\n",
        "discriminator_xx.eval()\n",
        "\n",
        "anomaly_scores = []\n",
        "y_true = []\n",
        "inference_times = []\n",
        "\n",
        "def compute_anomaly_score_combined(cnn_codes_orig, cnn_codes_rec, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Computes the anomaly score as a weighted combination of the L1 and L2 norms.\n",
        "    alpha: weight for L1 loss (between 0 and 1). (1-alpha) is the weight for L2 loss.\n",
        "    \"\"\"\n",
        "    l1_score = torch.mean(torch.abs(cnn_codes_orig - cnn_codes_rec), dim=1)\n",
        "    l2_score = torch.norm(cnn_codes_orig - cnn_codes_rec, p=2, dim=1)\n",
        "    # Combine the scores.\n",
        "    combined_score = alpha * l1_score + (1 - alpha) * l2_score\n",
        "    return combined_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_batch, labels in test_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1. Get CNN codes for original samples from Dxx.\n",
        "        #    Here, we assume model_Dxx(x) returns (logits, cnn_code)\n",
        "        _, cnn_codes_orig = discriminator_xx(x_batch,x_batch)\n",
        "\n",
        "        # 2. Compute reconstruction x_rec = G(E(x))\n",
        "        z = encoder(x_batch)\n",
        "        x_rec = generator(z)\n",
        "\n",
        "        # 3. Get CNN codes for reconstructed samples.\n",
        "        _, cnn_codes_rec = discriminator_xx(x_rec,x_rec)\n",
        "\n",
        "        # 4. Compute the L1 reconstruction error in the feature space (per sample).\n",
        "        #    Using mean absolute error (you could also use sum).\n",
        "        # batch_scores = torch.mean(torch.abs(cnn_codes_orig - cnn_codes_rec), dim=1)\n",
        "        # batch_scores = torch.norm(cnn_codes_orig - cnn_codes_rec, p=2, dim=1)\n",
        "        batch_scores = compute_anomaly_score_combined(cnn_codes_orig,cnn_codes_rec,alpha=0.3)\n",
        "\n",
        "        anomaly_scores.extend(batch_scores.cpu().numpy().tolist())\n",
        "        y_true.extend(labels.cpu().numpy().tolist())\n",
        "        # Record and store the inference time for this batch.\n",
        "        batch_inference_time = time.time() - start_time\n",
        "        inference_times.append(batch_inference_time)\n",
        "\n",
        "print(\"y_true: {}\".format(y_true[:5]))\n",
        "print(\"anomaly_scores: {}\".format(anomaly_scores[:5])) # Access the first element of the desired rows using slicing\n",
        "\n",
        "# Compute AUROC using the anomaly scores.\n",
        "auroc = roc_auc_score(y_true, anomaly_scores)\n",
        "print(\"AUROC: {:.4f}\".format(auroc))\n",
        "\n",
        "# Calculate average inference time over all batches.\n",
        "mean_inference_time = np.mean(inference_times)\n",
        "print(\"Mean inference time per batch: {:.4f} sec\".format(mean_inference_time))\n",
        "\n",
        "# Assume y_true is a list/array of true labels (0 for normal, 1 for anomaly)\n",
        "# and anomaly_scores is a list/array of your model's anomaly scores\n",
        "average_precision = average_precision_score(y_true, anomaly_scores)\n",
        "print(\"Average Precision (AUPRC): {:.4f}\".format(average_precision))\n"
      ],
      "metadata": {
        "id": "XMyxQkO3QNUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Suppose these are defined from your ALAD model inference:\n",
        "# y_true: true labels, e.g. np.array([...])\n",
        "# anomaly_scores: continuous anomaly scores, e.g. np.array([...])\n",
        "\n",
        "# Convert lists to numpy arrays if necessary.\n",
        "y_true = np.array(y_true)\n",
        "anomaly_scores = np.array(anomaly_scores)\n",
        "\n",
        "# Option 1: Determine threshold using the 95th percentile of normal samples.\n",
        "# (Assumes that normal samples are labeled 0.)\n",
        "normal_scores = anomaly_scores[y_true == 0]\n",
        "threshold = np.percentile(normal_scores, 95)\n",
        "print(\"Threshold based on 95th percentile of normal samples:\", threshold)\n",
        "\n",
        "# Option 2: Or set a manual threshold (uncomment below if needed).\n",
        "# threshold = 0.5\n",
        "\n",
        "# Generate binary predictions: predict fraud (1) if score > threshold, else normal (0)\n",
        "y_pred = (anomaly_scores > threshold).astype(int)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"Precision: {:.4f}\".format(precision))\n",
        "print(\"Recall: {:.4f}\".format(recall))\n",
        "print(\"F1 Score: {:.4f}\".format(f1))\n"
      ],
      "metadata": {
        "id": "GK98vlyz5aaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assume y_true and anomaly_scores are NumPy arrays\n",
        "# where y_true==0 for normal and y_true==1 for anomalies.\n",
        "\n",
        "\n",
        "normal_scores = anomaly_scores[y_true == 0]\n",
        "anomaly_scores_only = anomaly_scores[y_true == 1]\n",
        "\n",
        "plt.hist(normal_scores, bins=50, alpha=0.6, label='Normal')\n",
        "plt.hist(anomaly_scores_only, bins=50, alpha=0.6, label='Anomaly')\n",
        "plt.xlabel(\"Anomaly Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Anomaly Scores\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4PfQB52wCfsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_true, anomaly_scores)\n",
        "\n",
        "# Compute AUPRC (Area Under PR Curve)\n",
        "auprc = auc(recall, precision)  # Using AUC function\n",
        "ap_score = average_precision_score(y_true, anomaly_scores)  # Direct AUPRC score\n",
        "\n",
        "print(f\"AUPRC (using auc function): {auprc:.4f}\")\n",
        "print(f\"AUPRC (using average_precision_score): {ap_score:.4f}\")\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(recall, precision, marker='.', label=f'AUPRC = {auprc:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_kTeroCNSxEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}