{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHvrpQuTSENa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import hashlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load datasets\n",
        "train_transaction = pd.read_csv(\"/content/drive/MyDrive/cc_dataset/train_transaction.csv\")\n",
        "train_identity = pd.read_csv(\"/content/drive/MyDrive/cc_dataset/train_identity.csv\")\n",
        "\n",
        "\n",
        "# Merge transaction and identity data\n",
        "train_df = train_transaction.merge(train_identity, on=\"TransactionID\", how=\"left\")\n",
        "\n",
        "\n",
        "# Assume the dataset is loaded into a DataFrame called df\n",
        "# For example:\n",
        "# df = pd.read_csv('train_transaction.csv')\n",
        "\n",
        "# Define a reference date (the base date for TransactionDT)\n",
        "# Many use '2017-11-30' as the reference for this dataset\n",
        "start_date = pd.Timestamp('2017-11-30')\n",
        "\n",
        "# Convert TransactionDT (seconds elapsed) to a datetime column\n",
        "train_df['TransactionDT_datetime'] = pd.to_datetime(train_df['TransactionDT'], unit='s', origin=start_date)\n",
        "\n",
        "# Extract useful time-based features\n",
        "train_df['trans_hour'] = train_df['TransactionDT_datetime'].dt.hour            # Hour of the day\n",
        "train_df['trans_day'] = train_df['TransactionDT_datetime'].dt.day               # Day of the month\n",
        "train_df['trans_weekday'] = train_df['TransactionDT_datetime'].dt.dayofweek      # Day of the week (0=Monday, 6=Sunday)\n",
        "train_df['trans_month'] = train_df['TransactionDT_datetime'].dt.month           # Month\n",
        "train_df['trans_dayofyear'] = train_df['TransactionDT_datetime'].dt.dayofyear     # Day of the year\n",
        "\n",
        "# You might also want the elapsed days from the start date\n",
        "# train_df['elapsed_days'] = (train_df['TransactionDT_datetime'] - start_date).dt.days\n",
        "\n",
        "def generate_entity_id(row):\n",
        "    \"\"\"\n",
        "    Generate a unique entity identifier based on the card, address, time, and D1 features.\n",
        "    The features are concatenated into a single string and then hashed using MD5.\n",
        "    \"\"\"\n",
        "    # Convert each feature to string and concatenate them with a separator\n",
        "    fingerprint = f\"{row['card1']}_{row['card2']}_{row['card3']}_{row['card5']}_{row['addr1']}_{row['addr2']}\"\n",
        "    # Compute the MD5 hash of the fingerprint and return the hexadecimal digest\n",
        "    return hashlib.md5(fingerprint.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Assume df is your DataFrame containing the columns: 'card', 'address', 'time', and 'D1'\n",
        "# train_df['ENTITY_ID'] = train_df.apply(generate_entity_id, axis=1)\n",
        "\n",
        "# Display a few rows to verify the new features\n",
        "print(train_df[['TransactionDT', 'TransactionDT_datetime', 'trans_hour', 'trans_weekday','card1','addr1']].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "qIWTQ1YXSUUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ef3bd9-afc8-4b7a-9e25-55923e1ecd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   TransactionDT TransactionDT_datetime  trans_hour  trans_weekday  card1  \\\n",
            "0          86400    2017-12-01 00:00:00           0              4  13926   \n",
            "1          86401    2017-12-01 00:00:01           0              4   2755   \n",
            "2          86469    2017-12-01 00:01:09           0              4   4663   \n",
            "3          86499    2017-12-01 00:01:39           0              4  18132   \n",
            "4          86506    2017-12-01 00:01:46           0              4   4497   \n",
            "\n",
            "   addr1  \n",
            "0  315.0  \n",
            "1  325.0  \n",
            "2  330.0  \n",
            "3  476.0  \n",
            "4  420.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[['TransactionDT', 'TransactionDT_datetime', 'trans_hour', 'trans_weekday', 'elapsed_days','ENTITY_ID','card1','card2']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EWwEgGAszLq",
        "outputId": "8124d52f-9ba8-406e-f6b6-42a547cb411e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   TransactionDT TransactionDT_datetime  trans_hour  trans_weekday  \\\n",
            "0          86400    2017-12-01 00:00:00           0              4   \n",
            "1          86401    2017-12-01 00:00:01           0              4   \n",
            "2          86469    2017-12-01 00:01:09           0              4   \n",
            "3          86499    2017-12-01 00:01:39           0              4   \n",
            "4          86506    2017-12-01 00:01:46           0              4   \n",
            "\n",
            "   elapsed_days                         ENTITY_ID  card1  card2  \n",
            "0             1  6abdc24e927f42d73e31e8dfab05b0e7  13926    NaN  \n",
            "1             1  ea9290514570fe7875d8969f35c45fc3   2755  404.0  \n",
            "2             1  a64a962e3e9dcbf7df3d4b133d37d040   4663  490.0  \n",
            "3             1  b32384f80f08541077fbb79b526c4998  18132  567.0  \n",
            "4             1  2628345a7321877be43058b62f2b080d   4497  514.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_2 = train_df.drop(columns=[\"isFraud\"])\n",
        "# train_df_2 = train_df.drop(columns=[\"TransactionID\", \"isFraud\"])\n",
        "c = (train_df_2.dtypes == 'object')\n",
        "n = (train_df_2.dtypes != 'object')\n",
        "cat_id_cols = list(c[c].index)\n",
        "num_id_cols = list(n[n].index)\n",
        "\n",
        "print(cat_id_cols, \"\\n\")\n",
        "print(\"number categorical identity features: \", len(cat_id_cols), \"\\n\\n\")\n",
        "print(num_id_cols, \"\\n\")\n",
        "print(\"number numerical identity features: \", len(num_id_cols))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB_dJ8XI_Q99",
        "outputId": "2a6c323d-8fd9-4dc4-dfaf-92c1e6b612e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo'] \n",
            "\n",
            "number categorical identity features:  31 \n",
            "\n",
            "\n",
            "['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24', 'id_25', 'id_26', 'id_32', 'TransactionDT_datetime', 'trans_hour', 'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'] \n",
            "\n",
            "number numerical identity features:  408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_missing_cat_cols = []      # lower than 15% missing values\n",
        "medium_missing_cat_cols = []   # between 15% and 90% missing\n",
        "many_missing_cat_cols = []     # more than 90% missing\n",
        "\n",
        "for i in cat_id_cols:\n",
        "    percentage = train_df_2[i].isnull().sum() * 100 / len(train_df_2[i])\n",
        "    if percentage < 15:\n",
        "        low_missing_cat_cols.append(i)\n",
        "    elif percentage >= 15 and percentage < 60:\n",
        "        medium_missing_cat_cols.append(i)\n",
        "    else:\n",
        "        many_missing_cat_cols.append(i)\n",
        "\n",
        "print(\"cat_id_cols: \\n\\n\")\n",
        "print(\"number low missing: \", len(low_missing_cat_cols), \"\\n\")\n",
        "print(\"number medium missing: \", len(medium_missing_cat_cols), \"\\n\")\n",
        "print(\"number many missing: \", len(many_missing_cat_cols), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3BAqwMdFvz4",
        "outputId": "419663cc-f44e-49c1-a9eb-e187db93dffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat_id_cols: \n",
            "\n",
            "\n",
            "number low missing:  3 \n",
            "\n",
            "number medium missing:  10 \n",
            "\n",
            "number many missing:  18 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_missing_num_cols = []      # lower than 15% missing values\n",
        "medium_missing_num_cols = []   # between 15% and 60% missing\n",
        "many_missing_num_cols = []     # more than 60% missing\n",
        "\n",
        "for i in num_id_cols:\n",
        "    percentage = train_df_2[i].isnull().sum() * 100 / len(train_df_2[i])\n",
        "    if percentage < 15:\n",
        "        low_missing_num_cols.append(i)\n",
        "    elif percentage >= 15 and percentage < 60:\n",
        "        medium_missing_num_cols.append(i)\n",
        "    else:\n",
        "        many_missing_num_cols.append(i)\n",
        "\n",
        "print(\"num_id_cols: \\n\\n\")\n",
        "print(\"number low missing: \", len(low_missing_num_cols), \"\\n\")\n",
        "print(\"number medium missing: \", len(medium_missing_num_cols), \"\\n\")\n",
        "print(\"number many missing: \", len(many_missing_num_cols), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2d4wAadH3z7",
        "outputId": "2f075fea-8275-4c9b-dbf7-0b918955d5cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_id_cols: \n",
            "\n",
            "\n",
            "number low missing:  162 \n",
            "\n",
            "number medium missing:  56 \n",
            "\n",
            "number many missing:  190 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape before dropping num_trans_cols: \", train_df.shape, \"\\n\")\n",
        "train_df = train_df.drop(columns = many_missing_num_cols)\n",
        "train_df_2 = train_df_2.drop(columns = many_missing_num_cols)\n",
        "print(\"shape after dropping num_trans_cols: \", train_df.shape, \"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# because we dropped some numerical columns from the dataframe,\n",
        "# we must create the list 'num_trans_cols' and\n",
        "# 'num_id_cols' again such that the dropped cols are no longer in them\n",
        "n = (train_df_2.dtypes != 'object')\n",
        "num_trans_cols = list(n[n].index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9mufdmNJg4M",
        "outputId": "1c155947-35d2-4d26-ff5b-ef1e9726128c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape before dropping num_trans_cols:  (590540, 440) \n",
            "\n",
            "shape after dropping num_trans_cols:  (590540, 250) \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns = [\"TransactionDT_datetime\"])\n",
        "train_df_2 = train_df_2.drop(columns = [\"TransactionDT_datetime\"])\n",
        "low_missing_num_cols.remove(\"TransactionDT_datetime\")\n",
        "print(low_missing_num_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itGEeXcM_py7",
        "outputId": "685caf9c-206f-4454-f7f9-d4e06493ce6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D10', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'trans_hour', 'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Low missing values\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(\"index before imputation: \", train_df.index, \"\\n\")\n",
        "print(\"columns before imputation: \", train_df.columns, \"\\n\")\n",
        "\n",
        "print(\"starting imputation..... \\n\\n\")\n",
        "my_imputer = SimpleImputer(strategy = 'mean')\n",
        "my_imputer.fit(train_df[low_missing_num_cols])\n",
        "\n",
        "#print(\"values before imputing: \", train_transaction[low_missing_num_trans_cols], \"\\n\")\n",
        "\n",
        "train_df[low_missing_num_cols] = my_imputer.transform(train_df[low_missing_num_cols])\n",
        "\n",
        "print(\"index after imputation: \", train_df.index, \"\\n\")\n",
        "print(\"columns after imputation: \", train_df.columns, \"\\n\")\n",
        "\n",
        "print(\"values after imputing: \", train_df[low_missing_num_cols], \"\\n\")\n",
        "\n",
        "print(\"As we can see the imputation was successful! \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JjIVSgxLb-R",
        "outputId": "d7f7d031-f9ce-4d06-a966-35649c5e0542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index before imputation:  RangeIndex(start=0, stop=590540, step=1) \n",
            "\n",
            "columns before imputation:  Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
            "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
            "       ...\n",
            "       'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'trans_hour',\n",
            "       'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'],\n",
            "      dtype='object', length=249) \n",
            "\n",
            "starting imputation..... \n",
            "\n",
            "\n",
            "index after imputation:  RangeIndex(start=0, stop=590540, step=1) \n",
            "\n",
            "columns after imputation:  Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
            "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
            "       ...\n",
            "       'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'trans_hour',\n",
            "       'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'],\n",
            "      dtype='object', length=249) \n",
            "\n",
            "values after imputing:          TransactionID  TransactionDT  TransactionAmt    card1       card2  \\\n",
            "0           2987000.0        86400.0           68.50  13926.0  362.555488   \n",
            "1           2987001.0        86401.0           29.00   2755.0  404.000000   \n",
            "2           2987002.0        86469.0           59.00   4663.0  490.000000   \n",
            "3           2987003.0        86499.0           50.00  18132.0  567.000000   \n",
            "4           2987004.0        86506.0           50.00   4497.0  514.000000   \n",
            "...               ...            ...             ...      ...         ...   \n",
            "590535      3577535.0     15811047.0           49.00   6550.0  362.555488   \n",
            "590536      3577536.0     15811049.0           39.50  10444.0  225.000000   \n",
            "590537      3577537.0     15811079.0           30.95  12037.0  595.000000   \n",
            "590538      3577538.0     15811088.0          117.00   7826.0  481.000000   \n",
            "590539      3577539.0     15811131.0          279.95  15066.0  170.000000   \n",
            "\n",
            "        card3  card5  addr1  addr2   C1  ...    V317   V318        V319  \\\n",
            "0       150.0  142.0  315.0   87.0  1.0  ...   117.0    0.0    0.000000   \n",
            "1       150.0  102.0  325.0   87.0  1.0  ...     0.0    0.0    0.000000   \n",
            "2       150.0  166.0  330.0   87.0  1.0  ...     0.0    0.0    0.000000   \n",
            "3       150.0  117.0  476.0   87.0  2.0  ...  1404.0  790.0    0.000000   \n",
            "4       150.0  102.0  420.0   87.0  1.0  ...     0.0    0.0    0.000000   \n",
            "...       ...    ...    ...    ...  ...  ...     ...    ...         ...   \n",
            "590535  150.0  226.0  272.0   87.0  2.0  ...     0.0    0.0    0.000000   \n",
            "590536  150.0  224.0  204.0   87.0  1.0  ...     0.0    0.0    0.000000   \n",
            "590537  150.0  224.0  231.0   87.0  1.0  ...     0.0    0.0    0.000000   \n",
            "590538  150.0  224.0  387.0   87.0  1.0  ...  2234.0    0.0    0.000000   \n",
            "590539  150.0  102.0  299.0   87.0  2.0  ...     0.0    0.0  279.950012   \n",
            "\n",
            "              V320        V321  trans_hour  trans_day  trans_weekday  \\\n",
            "0         0.000000    0.000000         0.0        1.0            4.0   \n",
            "1         0.000000    0.000000         0.0        1.0            4.0   \n",
            "2         0.000000    0.000000         0.0        1.0            4.0   \n",
            "3         0.000000    0.000000         0.0        1.0            4.0   \n",
            "4         0.000000    0.000000         0.0        1.0            4.0   \n",
            "...            ...         ...         ...        ...            ...   \n",
            "590535    0.000000    0.000000        23.0       31.0            3.0   \n",
            "590536    0.000000    0.000000        23.0       31.0            3.0   \n",
            "590537    0.000000    0.000000        23.0       31.0            3.0   \n",
            "590538    0.000000    0.000000        23.0       31.0            3.0   \n",
            "590539  279.950012  279.950012        23.0       31.0            3.0   \n",
            "\n",
            "        trans_month  trans_dayofyear  \n",
            "0              12.0            335.0  \n",
            "1              12.0            335.0  \n",
            "2              12.0            335.0  \n",
            "3              12.0            335.0  \n",
            "4              12.0            335.0  \n",
            "...             ...              ...  \n",
            "590535          5.0            151.0  \n",
            "590536          5.0            151.0  \n",
            "590537          5.0            151.0  \n",
            "590538          5.0            151.0  \n",
            "590539          5.0            151.0  \n",
            "\n",
            "[590540 rows x 161 columns] \n",
            "\n",
            "As we can see the imputation was successful! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Medium missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(\"index before imputation: \", train_df.index, \"\\n\")\n",
        "print(\"columns before imputation: \", train_df.columns, \"\\n\")\n",
        "\n",
        "print(\"starting imputation..... \\n\\n\")\n",
        "my_imputer = SimpleImputer(strategy = 'median')\n",
        "my_imputer.fit(train_df[medium_missing_num_cols])\n",
        "\n",
        "#print(\"values before imputing: \", train_transaction[low_missing_num_trans_cols], \"\\n\")\n",
        "\n",
        "train_df[medium_missing_num_cols] = my_imputer.transform(train_df[medium_missing_num_cols])\n",
        "\n",
        "print(\"index after imputation: \", train_df.index, \"\\n\")\n",
        "print(\"columns after imputation: \", train_df.columns, \"\\n\")\n",
        "\n",
        "print(\"values after imputing: \", train_df[medium_missing_num_cols], \"\\n\")\n",
        "\n",
        "print(\"As we can see the imputation was successful! \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luBL_PVUMaZG",
        "outputId": "fd2166d8-a9c5-4098-d35c-81fde0f310b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index before imputation:  RangeIndex(start=0, stop=590540, step=1) \n",
            "\n",
            "columns before imputation:  Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
            "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
            "       ...\n",
            "       'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'trans_hour',\n",
            "       'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'],\n",
            "      dtype='object', length=249) \n",
            "\n",
            "starting imputation..... \n",
            "\n",
            "\n",
            "index after imputation:  RangeIndex(start=0, stop=590540, step=1) \n",
            "\n",
            "columns after imputation:  Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
            "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
            "       ...\n",
            "       'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'trans_hour',\n",
            "       'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'],\n",
            "      dtype='object', length=249) \n",
            "\n",
            "values after imputing:          dist1     D2    D3    D4    D5    D11    D15   V1   V2   V3  ...  V85  \\\n",
            "0        19.0   97.0  13.0  26.0  10.0   13.0    0.0  1.0  1.0  1.0  ...  0.0   \n",
            "1         8.0   97.0   8.0   0.0  10.0   43.0    0.0  1.0  1.0  1.0  ...  0.0   \n",
            "2       287.0   97.0   8.0   0.0  10.0  315.0  315.0  1.0  1.0  1.0  ...  0.0   \n",
            "3         8.0  112.0   0.0  94.0   0.0   43.0  111.0  1.0  1.0  1.0  ...  0.0   \n",
            "4         8.0   97.0   8.0  26.0  10.0   43.0   52.0  1.0  1.0  1.0  ...  0.0   \n",
            "...       ...    ...   ...   ...   ...    ...    ...  ...  ...  ...  ...  ...   \n",
            "590535   48.0   29.0  30.0  26.0  10.0   56.0   56.0  1.0  1.0  1.0  ...  0.0   \n",
            "590536    8.0   97.0   8.0   0.0  10.0    0.0    0.0  1.0  1.0  1.0  ...  0.0   \n",
            "590537    8.0   97.0   8.0   0.0  10.0    0.0    0.0  1.0  1.0  1.0  ...  0.0   \n",
            "590538    3.0   22.0   0.0  22.0   0.0   22.0   22.0  1.0  1.0  1.0  ...  0.0   \n",
            "590539    8.0   97.0   0.0   1.0   0.0    0.0    1.0  1.0  1.0  1.0  ...  0.0   \n",
            "\n",
            "        V86  V87  V88  V89  V90  V91  V92  V93  V94  \n",
            "0       1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "1       1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "2       1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "3       1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "4       1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
            "590535  1.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
            "590536  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "590537  1.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
            "590538  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "590539  2.0  2.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
            "\n",
            "[590540 rows x 56 columns] \n",
            "\n",
            "As we can see the imputation was successful! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trans_cols.remove(\"TransactionDT_datetime\")\n",
        "print(num_trans_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WmRDIVWAAlK",
        "outputId": "4d7d8c07-786f-4ac5-d96a-014807aaba06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'trans_hour', 'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[num_trans_cols].isnull().sum().sum())\n",
        "print(\"train_df.memory_usage(): \", train_df.info(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "991x3StyMa-t",
        "outputId": "18463e26-c543-4df2-d8c0-6916dc944198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 590540 entries, 0 to 590539\n",
            "Columns: 431 entries, TransactionID to ENTITY_ID\n",
            "dtypes: float64(398), int64(1), object(32)\n",
            "memory usage: 1.9+ GB\n",
            "train_df.memory_usage():  None \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "object_counter = 0\n",
        "int_counter = 0\n",
        "float_counter = 0\n",
        "\n",
        "not_detected = []\n",
        "\n",
        "for i in train_df.columns:\n",
        "        if train_df[i].dtype == 'object':\n",
        "            object_counter += 1\n",
        "        elif train_df[i].dtype == 'int':\n",
        "            int_counter += 1\n",
        "        elif train_df[i].dtype in ['float', 'float16', 'float32', 'float64']:\n",
        "            float_counter += 1\n",
        "        else:\n",
        "            not_detected.append(i)\n",
        "\n",
        "print(\"transaction_data has \", \"\\n\")\n",
        "print(object_counter, \"object columns, \\n\")\n",
        "print(int_counter, \"int columns, \\n\")\n",
        "print(float_counter, \"float columns \\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-NtIKz5OQkq",
        "outputId": "d243a35e-8198-4b37-cb75-075169b0d93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transaction_data has  \n",
            "\n",
            "31 object columns, \n",
            "\n",
            "1 int columns, \n",
            "\n",
            "217 float columns \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape before dropping many_missing_cat_trans_cols: \", train_df.shape, \"\\n\")\n",
        "train_df = train_df.drop(columns = many_missing_cat_cols)\n",
        "train_df_2 = train_df_2.drop(columns = many_missing_cat_cols)\n",
        "print(\"shape after dropping many_missing_cat_trans_cols: \", train_df.shape, \"\\n\\n\")\n",
        "\n",
        "\n",
        "# because we dropped some categorical columns from the dataframe,\n",
        "# we must create the list 'cat_trans_cols' and\n",
        "# 'cat_id_cols' again such that the dropped cols are no longer in them\n",
        "c = (train_df.dtypes == 'object')\n",
        "cat_trans_cols = list(c[c].index)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adu7pFNLO-j9",
        "outputId": "10d49eeb-1c2a-47aa-8c05-5754746b483d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape before dropping many_missing_cat_trans_cols:  (590540, 249) \n",
            "\n",
            "shape after dropping many_missing_cat_trans_cols:  (590540, 231) \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in cat_trans_cols:\n",
        "    print(col, train_df[col].nunique(), \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiQoxnDBQ0Em",
        "outputId": "18728168-cd7c-4853-e060-ec6a45a3d613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProductCD 5 \n",
            "\n",
            "card4 4 \n",
            "\n",
            "card6 4 \n",
            "\n",
            "P_emaildomain 59 \n",
            "\n",
            "M1 2 \n",
            "\n",
            "M2 2 \n",
            "\n",
            "M3 2 \n",
            "\n",
            "M4 3 \n",
            "\n",
            "M5 2 \n",
            "\n",
            "M6 2 \n",
            "\n",
            "M7 2 \n",
            "\n",
            "M8 2 \n",
            "\n",
            "M9 2 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_card_trans_cols_1 = []\n",
        "high_card_trans_cols_1 = []\n",
        "\n",
        "for i in cat_trans_cols:\n",
        "    if train_df[i].nunique() < 6:\n",
        "        low_card_trans_cols_1.append(i)\n",
        "    else:\n",
        "        high_card_trans_cols_1.append(i)\n",
        "\n",
        "print(low_card_trans_cols_1)\n",
        "print(high_card_trans_cols_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhsLxFhxRGvr",
        "outputId": "2201095e-b75a-4e84-d190-e40b5acba8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
            "['P_emaildomain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in cat_trans_cols:\n",
        "    most_frequent_value = train_df[i].mode()[0]\n",
        "    print(\"For column: \", i, \"the most frequent value is: \", most_frequent_value, \"\\n\")\n",
        "    train_df[i].fillna(most_frequent_value, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nteNbA5tQZ4f",
        "outputId": "80b150c5-7aae-40ab-99ec-45a991842355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For column:  ProductCD the most frequent value is:  W \n",
            "\n",
            "For column:  card4 the most frequent value is:  visa \n",
            "\n",
            "For column:  card6 the most frequent value is:  debit \n",
            "\n",
            "For column:  P_emaildomain the most frequent value is:  gmail.com \n",
            "\n",
            "For column:  M1 the most frequent value is:  T \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-ae4b4beae7cf>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df[i].fillna(most_frequent_value, inplace = True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For column:  M2 the most frequent value is:  T \n",
            "\n",
            "For column:  M3 the most frequent value is:  T \n",
            "\n",
            "For column:  M4 the most frequent value is:  M0 \n",
            "\n",
            "For column:  M5 the most frequent value is:  F \n",
            "\n",
            "For column:  M6 the most frequent value is:  F \n",
            "\n",
            "For column:  M7 the most frequent value is:  F \n",
            "\n",
            "For column:  M8 the most frequent value is:  F \n",
            "\n",
            "For column:  M9 the most frequent value is:  T \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "print(\"transaction_data.shape before label-encoding: \", train_df.shape, \"\\n\")\n",
        "for col in high_card_trans_cols_1:\n",
        "  train_df[col] = label_encoder.fit_transform(train_df[col])\n",
        "\n",
        "print(\"transaction_data.shape after label-encoding: \", train_df.shape, \"\\n\")\n",
        "print(\"transaction_data[high_card_trans_cols] after label_encoding: \",train_df[high_card_trans_cols_1], \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj9iGfvHUzbE",
        "outputId": "9f24f772-d2ce-4186-a53c-fd97c2fc1250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transaction_data.shape before label-encoding:  (590540, 231) \n",
            "\n",
            "transaction_data.shape after label-encoding:  (590540, 231) \n",
            "\n",
            "transaction_data[high_card_trans_cols] after label_encoding:          P_emaildomain\n",
            "0                  16\n",
            "1                  16\n",
            "2                  35\n",
            "3                  53\n",
            "4                  16\n",
            "...               ...\n",
            "590535             16\n",
            "590536             16\n",
            "590537             16\n",
            "590538              2\n",
            "590539             16\n",
            "\n",
            "[590540 rows x 1 columns] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape before encoding: \", train_df.shape, \"\\n\")\n",
        "print(\"columns to encode: \", low_card_trans_cols_1, \"\\n\")\n",
        "print(\"transaction_data.columns.to_list() before encoding: \", train_df.columns.to_list(), \"\\n\")\n",
        "\n",
        "# this line does the onehot encoding\n",
        "low_card_trans_encoded = pd.get_dummies(train_df[low_card_trans_cols_1], dummy_na = False)\n",
        "\n",
        "low_card_trans_encoded = low_card_trans_encoded.astype(int)\n",
        "print(\"shape after encoding: \", train_df.shape, \"\\n\\n\")\n",
        "print(\"shape of new dataframe: \", low_card_trans_encoded.shape, \"\\n\\n\")\n",
        "print(\"newly generated columns: \", low_card_trans_encoded.columns, \"\\n\")\n",
        "print(\"low_card_trans_encoded.info(): \", low_card_trans_encoded.info(),\"\\n\")\n",
        "print(\"transaction_data.columns.to_list() after encoding: \", train_df.columns.to_list(), \"\\n\")\n",
        "print(\"low_card_trans_encoded: \", low_card_trans_encoded.head(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0fjxtf6YNbg",
        "outputId": "a361ffe4-da39-4f78-a711-b503778b6ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape before encoding:  (590540, 231) \n",
            "\n",
            "columns to encode:  ['ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'] \n",
            "\n",
            "transaction_data.columns.to_list() before encoding:  ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'P_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'trans_hour', 'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'] \n",
            "\n",
            "shape after encoding:  (590540, 231) \n",
            "\n",
            "\n",
            "shape of new dataframe:  (590540, 32) \n",
            "\n",
            "\n",
            "newly generated columns:  Index(['ProductCD_C', 'ProductCD_H', 'ProductCD_R', 'ProductCD_S',\n",
            "       'ProductCD_W', 'card4_american express', 'card4_discover',\n",
            "       'card4_mastercard', 'card4_visa', 'card6_charge card', 'card6_credit',\n",
            "       'card6_debit', 'card6_debit or credit', 'M1_F', 'M1_T', 'M2_F', 'M2_T',\n",
            "       'M3_F', 'M3_T', 'M4_M0', 'M4_M1', 'M4_M2', 'M5_F', 'M5_T', 'M6_F',\n",
            "       'M6_T', 'M7_F', 'M7_T', 'M8_F', 'M8_T', 'M9_F', 'M9_T'],\n",
            "      dtype='object') \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 590540 entries, 0 to 590539\n",
            "Data columns (total 32 columns):\n",
            " #   Column                  Non-Null Count   Dtype\n",
            "---  ------                  --------------   -----\n",
            " 0   ProductCD_C             590540 non-null  int64\n",
            " 1   ProductCD_H             590540 non-null  int64\n",
            " 2   ProductCD_R             590540 non-null  int64\n",
            " 3   ProductCD_S             590540 non-null  int64\n",
            " 4   ProductCD_W             590540 non-null  int64\n",
            " 5   card4_american express  590540 non-null  int64\n",
            " 6   card4_discover          590540 non-null  int64\n",
            " 7   card4_mastercard        590540 non-null  int64\n",
            " 8   card4_visa              590540 non-null  int64\n",
            " 9   card6_charge card       590540 non-null  int64\n",
            " 10  card6_credit            590540 non-null  int64\n",
            " 11  card6_debit             590540 non-null  int64\n",
            " 12  card6_debit or credit   590540 non-null  int64\n",
            " 13  M1_F                    590540 non-null  int64\n",
            " 14  M1_T                    590540 non-null  int64\n",
            " 15  M2_F                    590540 non-null  int64\n",
            " 16  M2_T                    590540 non-null  int64\n",
            " 17  M3_F                    590540 non-null  int64\n",
            " 18  M3_T                    590540 non-null  int64\n",
            " 19  M4_M0                   590540 non-null  int64\n",
            " 20  M4_M1                   590540 non-null  int64\n",
            " 21  M4_M2                   590540 non-null  int64\n",
            " 22  M5_F                    590540 non-null  int64\n",
            " 23  M5_T                    590540 non-null  int64\n",
            " 24  M6_F                    590540 non-null  int64\n",
            " 25  M6_T                    590540 non-null  int64\n",
            " 26  M7_F                    590540 non-null  int64\n",
            " 27  M7_T                    590540 non-null  int64\n",
            " 28  M8_F                    590540 non-null  int64\n",
            " 29  M8_T                    590540 non-null  int64\n",
            " 30  M9_F                    590540 non-null  int64\n",
            " 31  M9_T                    590540 non-null  int64\n",
            "dtypes: int64(32)\n",
            "memory usage: 144.2 MB\n",
            "low_card_trans_encoded.info():  None \n",
            "\n",
            "transaction_data.columns.to_list() after encoding:  ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'P_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'trans_hour', 'trans_day', 'trans_weekday', 'trans_month', 'trans_dayofyear'] \n",
            "\n",
            "low_card_trans_encoded:     ProductCD_C  ProductCD_H  ProductCD_R  ProductCD_S  ProductCD_W  \\\n",
            "0            0            0            0            0            1   \n",
            "1            0            0            0            0            1   \n",
            "2            0            0            0            0            1   \n",
            "3            0            0            0            0            1   \n",
            "4            0            1            0            0            0   \n",
            "\n",
            "   card4_american express  card4_discover  card4_mastercard  card4_visa  \\\n",
            "0                       0               1                 0           0   \n",
            "1                       0               0                 1           0   \n",
            "2                       0               0                 0           1   \n",
            "3                       0               0                 1           0   \n",
            "4                       0               0                 1           0   \n",
            "\n",
            "   card6_charge card  ...  M5_F  M5_T  M6_F  M6_T  M7_F  M7_T  M8_F  M8_T  \\\n",
            "0                  0  ...     1     0     0     1     1     0     1     0   \n",
            "1                  0  ...     0     1     0     1     1     0     1     0   \n",
            "2                  0  ...     1     0     1     0     1     0     1     0   \n",
            "3                  0  ...     0     1     1     0     1     0     1     0   \n",
            "4                  0  ...     1     0     1     0     1     0     1     0   \n",
            "\n",
            "   M9_F  M9_T  \n",
            "0     0     1  \n",
            "1     0     1  \n",
            "2     1     0  \n",
            "3     0     1  \n",
            "4     0     1  \n",
            "\n",
            "[5 rows x 32 columns] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"transaction_data.shape before concatting: \", train_df.shape, \"\\n\")\n",
        "print(\"low_card_trans_encoded.shape before concatting: \", low_card_trans_encoded.shape, \"\\n\")\n",
        "\n",
        "train_concatted = pd.concat([train_df, low_card_trans_encoded], axis = 1)\n",
        "train_concatted.drop(columns = low_card_trans_cols_1, inplace = True)\n",
        "\n",
        "print(\"train_concatted.shape after concatting: \", train_concatted.shape, \"\\n\")\n",
        "print(\"train_concatted.columns after concatting: \", train_concatted.columns, \"\\n\")\n",
        "\n",
        "#del low_card_trans_encoded\n",
        "#del transaction_data\n",
        "\n",
        "print(train_concatted.info())\n",
        "train_concatted.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "1KrDs_tpZex4",
        "outputId": "542b6c68-1fad-4b2d-99fe-91fa6bca2037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transaction_data.shape before concatting:  (590540, 231) \n",
            "\n",
            "low_card_trans_encoded.shape before concatting:  (590540, 32) \n",
            "\n",
            "train_concatted.shape after concatting:  (590540, 251) \n",
            "\n",
            "train_concatted.columns after concatting:  Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'card1',\n",
            "       'card2', 'card3', 'card5', 'addr1', 'addr2',\n",
            "       ...\n",
            "       'M5_F', 'M5_T', 'M6_F', 'M6_T', 'M7_F', 'M7_T', 'M8_F', 'M8_T', 'M9_F',\n",
            "       'M9_T'],\n",
            "      dtype='object', length=251) \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 590540 entries, 0 to 590539\n",
            "Columns: 251 entries, TransactionID to M9_T\n",
            "dtypes: float64(217), int64(34)\n",
            "memory usage: 1.1 GB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  TransactionAmt    card1       card2  \\\n",
              "0      2987000.0        0        86400.0            68.5  13926.0  362.555488   \n",
              "1      2987001.0        0        86401.0            29.0   2755.0  404.000000   \n",
              "2      2987002.0        0        86469.0            59.0   4663.0  490.000000   \n",
              "3      2987003.0        0        86499.0            50.0  18132.0  567.000000   \n",
              "4      2987004.0        0        86506.0            50.0   4497.0  514.000000   \n",
              "\n",
              "   card3  card5  addr1  addr2  ...  M5_F  M5_T  M6_F  M6_T  M7_F  M7_T  M8_F  \\\n",
              "0  150.0  142.0  315.0   87.0  ...     1     0     0     1     1     0     1   \n",
              "1  150.0  102.0  325.0   87.0  ...     0     1     0     1     1     0     1   \n",
              "2  150.0  166.0  330.0   87.0  ...     1     0     1     0     1     0     1   \n",
              "3  150.0  117.0  476.0   87.0  ...     0     1     1     0     1     0     1   \n",
              "4  150.0  102.0  420.0   87.0  ...     1     0     1     0     1     0     1   \n",
              "\n",
              "   M8_T  M9_F  M9_T  \n",
              "0     0     0     1  \n",
              "1     0     0     1  \n",
              "2     0     1     0  \n",
              "3     0     0     1  \n",
              "4     0     0     1  \n",
              "\n",
              "[5 rows x 251 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ca9d8c6-3268-4488-bad2-d88aee29e369\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>...</th>\n",
              "      <th>M5_F</th>\n",
              "      <th>M5_T</th>\n",
              "      <th>M6_F</th>\n",
              "      <th>M6_T</th>\n",
              "      <th>M7_F</th>\n",
              "      <th>M7_T</th>\n",
              "      <th>M8_F</th>\n",
              "      <th>M8_T</th>\n",
              "      <th>M9_F</th>\n",
              "      <th>M9_T</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000.0</td>\n",
              "      <td>0</td>\n",
              "      <td>86400.0</td>\n",
              "      <td>68.5</td>\n",
              "      <td>13926.0</td>\n",
              "      <td>362.555488</td>\n",
              "      <td>150.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001.0</td>\n",
              "      <td>0</td>\n",
              "      <td>86401.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2755.0</td>\n",
              "      <td>404.000000</td>\n",
              "      <td>150.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002.0</td>\n",
              "      <td>0</td>\n",
              "      <td>86469.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>4663.0</td>\n",
              "      <td>490.000000</td>\n",
              "      <td>150.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003.0</td>\n",
              "      <td>0</td>\n",
              "      <td>86499.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>18132.0</td>\n",
              "      <td>567.000000</td>\n",
              "      <td>150.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004.0</td>\n",
              "      <td>0</td>\n",
              "      <td>86506.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>4497.0</td>\n",
              "      <td>514.000000</td>\n",
              "      <td>150.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  251 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ca9d8c6-3268-4488-bad2-d88aee29e369')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ca9d8c6-3268-4488-bad2-d88aee29e369 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ca9d8c6-3268-4488-bad2-d88aee29e369');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9a05e824-57a1-4dd2-8086-5644b2c36bff\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a05e824-57a1-4dd2-8086-5644b2c36bff')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9a05e824-57a1-4dd2-8086-5644b2c36bff button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_concatted"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_concatted.to_csv(\"/content/drive/MyDrive/ieeecis/train_concatted_v2.csv\")\n",
        "# Save the list to a CSV file\n",
        "pd.DataFrame(num_trans_cols).to_csv(\"/content/drive/MyDrive/ieeecis/num_trans_cols_v2.csv\", index=False, header=False)\n",
        "\n",
        "# # ... (Later, to load the list)\n",
        "\n",
        "# # Load the list from the CSV file\n",
        "# num_trans_cols = pd.read_csv(\"/content/drive/MyDrive/ieeecis/num_trans_cols.csv\", header=None).iloc[:, 0].tolist()"
      ],
      "metadata": {
        "id": "dkP8gLPuybPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_concatted = pd.read_csv(\"/content/drive/MyDrive/ieeecis/train_concatted.csv\")\n",
        "num_trans_cols = pd.read_csv(\"/content/drive/MyDrive/ieeecis/num_trans_cols.csv\", header=None).iloc[:, 0].tolist()"
      ],
      "metadata": {
        "id": "u82CgeFjztoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate normal & fraudulent samples\n",
        "X_normal = train_concatted[train_concatted['isFraud'] == 0]  # Normal transactions\n",
        "X_anomaly = train_concatted[train_concatted['isFraud'] == 1]  # Fraudulent transactions (for evaluation)\n",
        "\n",
        "print(f\"Train shape: {X_normal.shape}, Test shape: {X_anomaly.shape}\")\n",
        "\n",
        "# Drop TransactionID and target variable from train dataset for preprocessing\n",
        "# train_target = train_transaction[\"isFraud\"]\n",
        "X_normal.drop(columns=[\"TransactionID\", \"isFraud\"], inplace=True)\n",
        "X_anomaly.drop(columns=[\"TransactionID\",\"isFraud\"], inplace=True)\n",
        "# X_normal.drop(columns=[\"isFraud\"], inplace=True)\n",
        "# X_anomaly.drop(columns=[\"isFraud\"], inplace=True)\n",
        "\n",
        "num_trans_cols.remove(\"TransactionID\")\n",
        "\n",
        "# Normalize numerical features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_normal[num_trans_cols] = scaler.fit_transform(X_normal[num_trans_cols])\n",
        "X_anomaly[num_trans_cols] = scaler.transform(X_anomaly[num_trans_cols])\n",
        "\n",
        "\n",
        "# Convert data types to reduce memory usage\n",
        "def reduce_memory(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == \"float64\":\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "        elif df[col].dtype == \"int64\":\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "    return df\n",
        "\n",
        "#train_df = reduce_memory(train_df)\n",
        "#test_df = reduce_memory(test_df)\n",
        "# Final processed datasets\n",
        "print(f\"Train shape: {X_normal.shape}, Test shape: {X_anomaly.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6NLg8U9_FIy",
        "outputId": "ef77f508-21d2-4e50-dc48-4b6bb465f7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (569877, 464), Test shape: (20663, 464)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1c8cff8eda03>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_normal.drop(columns=[\"TransactionID\", \"isFraud\"], inplace=True)\n",
            "<ipython-input-4-1c8cff8eda03>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_anomaly.drop(columns=[\"TransactionID\",\"isFraud\"], inplace=True)\n",
            "<ipython-input-4-1c8cff8eda03>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_normal[num_trans_cols] = scaler.fit_transform(X_normal[num_trans_cols])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (569877, 462), Test shape: (20663, 462)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1c8cff8eda03>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_anomaly[num_trans_cols] = scaler.transform(X_anomaly[num_trans_cols])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torch.nn.utils.parametrizations as param\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Generator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "        # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.relu(self.fc1(z))\n",
        "        z = F.relu(self.fc2(z))\n",
        "        output = self.fc3(z)  # Last layer (no activation)\n",
        "        return output\n",
        "\n",
        "# PyTorch Encoder Model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, latent_dim)\n",
        "\n",
        "        # Xavier initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x),negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.fc2(x),negative_slope=0.2)\n",
        "        x = self.fc3(x)  # Last layer (latent space) has no activation\n",
        "        return x\n",
        "\n",
        "# Define Discriminator Dxz\n",
        "class DiscriminatorXZ(nn.Module):\n",
        "    def __init__(self, x_dim, z_dim, do_spectral_norm=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_dim (int): Dimensionality of the x input.\n",
        "            z_dim (int): Dimensionality of the z input.\n",
        "            do_spectral_norm (bool): If True, apply spectral normalization to linear layers.\n",
        "        \"\"\"\n",
        "        super(DiscriminatorXZ, self).__init__()\n",
        "\n",
        "        # Helper: apply spectral normalization if desired\n",
        "        sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda layer: layer\n",
        "\n",
        "        # D(x) branch: dense layer -> batch norm -> leaky ReLU\n",
        "        self.x_fc1 = sn(nn.Linear(x_dim, 128))\n",
        "        self.x_bn1 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # D(z) branch: dense layer -> leaky ReLU -> dropout\n",
        "        self.z_fc1 = sn(nn.Linear(z_dim, 128))\n",
        "        self.dropout = nn.Dropout(0.5)  # dropout rate 0.5\n",
        "\n",
        "        # Combined branch (D(x,z)): after concatenation of x and z branches\n",
        "        self.y_fc1 = sn(nn.Linear(128 + 128, 256))  # concatenated size = 256\n",
        "        self.y_fc2 = sn(nn.Linear(256, 1))  # output logits\n",
        "\n",
        "        # Xavier (Glorot) initialization for all linear layers\n",
        "        nn.init.xavier_uniform_(self.x_fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.z_fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.y_fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.y_fc2.weight)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        # D(x) branch:\n",
        "        x_out = self.x_fc1(x)\n",
        "        x_out = self.x_bn1(x_out)\n",
        "        x_out = F.leaky_relu(x_out,negative_slope=0.2)\n",
        "\n",
        "        # D(z) branch:\n",
        "        z_out = self.z_fc1(z)\n",
        "        z_out = F.leaky_relu(z_out,negative_slope=0.2)\n",
        "        z_out = self.dropout(z_out)  # dropout is active only in training mode\n",
        "\n",
        "        # Concatenate the branches along the feature dimension\n",
        "        y = torch.cat([x_out, z_out], dim=1)\n",
        "\n",
        "        # Combined branch:\n",
        "        y = self.y_fc1(y)\n",
        "        y = F.leaky_relu(y,negative_slope=0.2)\n",
        "        y = self.dropout(y)\n",
        "\n",
        "        intermediate_layer = y  # For feature matching\n",
        "\n",
        "        # Final logits layer (no activation)\n",
        "        logits = self.y_fc2(y)\n",
        "\n",
        "        return logits, intermediate_layer\n",
        "\n",
        "\n",
        "# Define Discriminator Dxx\n",
        "class DiscriminatorXX(nn.Module):\n",
        "    def __init__(self, input_dim, do_spectral_norm=True):\n",
        "        super(DiscriminatorXX, self).__init__()\n",
        "\n",
        "        # Apply spectral normalization if enabled\n",
        "        spectral_layer = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "        # Fully connected layers with Spectral Normalization\n",
        "        self.fc1 = spectral_layer(nn.Linear(input_dim * 2, 256))\n",
        "        self.fc2 = spectral_layer(nn.Linear(256, 128))\n",
        "        self.fc3 = spectral_layer(nn.Linear(128, 1))  # Final output layer\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
        "\n",
        "        # Xavier Initialization (equivalent to TensorFlow's `init_kernel`)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x, rec_x):\n",
        "        # Concatenate x and rec_x\n",
        "        net = torch.cat([x, rec_x], dim=1)\n",
        "\n",
        "        # Layer 1\n",
        "        net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "        net = self.dropout(net) if self.training else net  # Dropout only during training\n",
        "\n",
        "        # Layer 2\n",
        "        net = F.leaky_relu(self.fc2(net),negative_slope=0.2)\n",
        "        net = self.dropout(net) if self.training else net  # Dropout only during training\n",
        "\n",
        "        intermediate_layer = net  # Save intermediate layer for feature matching\n",
        "\n",
        "        # Layer 3 (Logits)\n",
        "        logits = self.fc3(net)  # No activation in final layer\n",
        "\n",
        "        return logits, intermediate_layer\n",
        "\n",
        "# Define Discriminator Dzz\n",
        "class DiscriminatorZZ(nn.Module):\n",
        "    def __init__(self, latent_dim, do_spectral_norm=False):\n",
        "        super(DiscriminatorZZ, self).__init__()\n",
        "\n",
        "        # If spectral normalization is desired, wrap the linear layers with it.\n",
        "        sn = torch.nn.utils.spectral_norm if do_spectral_norm else lambda x: x\n",
        "\n",
        "        # First layer: input dimension is latent_dim * 2 due to concatenation of z and rec_z.\n",
        "        self.fc1 = sn(nn.Linear(latent_dim * 2, 64))\n",
        "        # Second layer.\n",
        "        self.fc2 = sn(nn.Linear(64, 32))\n",
        "        # Third (output) layer: produces logits.\n",
        "        self.fc3 = sn(nn.Linear(32, 1))\n",
        "        # Dropout layer with rate 0.2.\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Xavier initialization (Glorot Uniform)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, z, rec_z):\n",
        "        # Concatenate along the feature dimension.\n",
        "        net = torch.cat([z, rec_z], dim=1)\n",
        "\n",
        "        # Layer 1: Dense -> Leaky ReLU -> Dropout.\n",
        "        net = F.leaky_relu(self.fc1(net),negative_slope=0.2)\n",
        "        net = self.dropout(net)  # Dropout is active only in training mode.\n",
        "\n",
        "        # Layer 2: Dense -> Leaky ReLU -> Dropout.\n",
        "        net = F.leaky_relu(self.fc2(net),negative_slope=0.2)\n",
        "        net = self.dropout(net)\n",
        "\n",
        "        # Save intermediate layer for feature matching.\n",
        "        intermediate_layer = net\n",
        "\n",
        "        # Layer 3: Dense to produce logits (no activation).\n",
        "        logits = self.fc3(net)\n",
        "        return logits, intermediate_layer\n",
        "\n"
      ],
      "metadata": {
        "id": "1ov8dIw95bJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "class DiscriminatorXZ_1(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        \"\"\"\n",
        "        Improved Dz discriminator that takes the concatenated input and latent vector,\n",
        "        and returns both the final logits and an intermediate feature representation.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of input x.\n",
        "            latent_dim (int): Dimensionality of latent vector z.\n",
        "        \"\"\"\n",
        "        super(DiscriminatorXZ_1, self).__init__()\n",
        "\n",
        "        # The network takes a concatenated vector of dimension (input_dim + latent_dim)\n",
        "        self.fc1 = spectral_norm(nn.Linear(input_dim + latent_dim, 512))\n",
        "        self.fc2 = spectral_norm(nn.Linear(512, 256))\n",
        "        self.fc3 = spectral_norm(nn.Linear(256, 128))  # Intermediate feature layer\n",
        "        self.fc4 = spectral_norm(nn.Linear(128, 1))    # Final logits layer\n",
        "\n",
        "        # Activation function: LeakyReLU with a small negative slope\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        # Concatenate the input and latent vector along the feature dimension\n",
        "        xz = torch.cat([x, z], dim=1)\n",
        "\n",
        "        out = self.fc1(xz)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Use the output of the third layer as intermediate features\n",
        "        intermediate = self.fc3(out)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "\n",
        "        logits = self.fc4(intermediate)\n",
        "        # If you plan to use BCEWithLogitsLoss, don't apply Sigmoid here.\n",
        "        return logits, intermediate\n",
        "\n",
        "\n",
        "class DiscriminatorXX_1(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Discriminator for comparing two x inputs.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of the input x.\n",
        "        \"\"\"\n",
        "        super(DiscriminatorXX_1, self).__init__()\n",
        "        # Input is a concatenation of two x vectors: dimension = 2 * input_dim\n",
        "        self.fc1 = spectral_norm(nn.Linear(2 * input_dim, 512))\n",
        "        self.fc2 = spectral_norm(nn.Linear(512, 256))\n",
        "        self.fc3 = spectral_norm(nn.Linear(256, 128))  # Intermediate features\n",
        "        self.fc4 = spectral_norm(nn.Linear(128, 1))    # Logits output\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Concatenate the two x vectors along the feature dimension\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        out = self.leaky_relu(self.fc1(x))\n",
        "        out = self.dropout(out)\n",
        "        out = self.leaky_relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        intermediate = self.leaky_relu(self.fc3(out))\n",
        "        logits = self.fc4(intermediate)\n",
        "        return logits, intermediate\n",
        "\n",
        "\n",
        "class DiscriminatorZZ_1(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        \"\"\"\n",
        "        Discriminator for comparing two latent vectors.\n",
        "\n",
        "        Args:\n",
        "            latent_dim (int): Dimensionality of the latent vector z.\n",
        "        \"\"\"\n",
        "        super(DiscriminatorZZ_1, self).__init__()\n",
        "        # Input is a concatenation of two z vectors: dimension = 2 * latent_dim\n",
        "        self.fc1 = spectral_norm(nn.Linear(2 * latent_dim, 64))\n",
        "        # self.fc2 = spectral_norm(nn.Linear(256, 128))\n",
        "        self.fc3 = spectral_norm(nn.Linear(64, 32))   # Intermediate features\n",
        "        self.fc4 = spectral_norm(nn.Linear(32, 1))     # Logits output\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        # Concatenate the two latent vectors\n",
        "        z = torch.cat([z1, z2], dim=1)\n",
        "        out = self.leaky_relu(self.fc1(z))\n",
        "        out = self.dropout(out)\n",
        "        # out = self.leaky_relu(self.fc2(out))\n",
        "        # out = self.dropout(out)\n",
        "        intermediate = self.leaky_relu(self.fc3(out))\n",
        "        logits = self.fc4(intermediate)\n",
        "        return logits, intermediate\n",
        "\n",
        "class Encoder_1(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        \"\"\"\n",
        "        Encoder that maps input x to latent vector z.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of input x.\n",
        "            latent_dim (int): Dimensionality of latent space z.\n",
        "        \"\"\"\n",
        "        super(Encoder_1, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, latent_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        z = self.fc2(out)\n",
        "        return z\n",
        "\n",
        "class Generator_1(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Generator that maps latent vector z to reconstructed input x_rec.\n",
        "\n",
        "        Args:\n",
        "            latent_dim (int): Dimensionality of latent space z.\n",
        "            output_dim (int): Dimensionality of output x (should match input_dim).\n",
        "        \"\"\"\n",
        "        super(Generator_1, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.relu(self.fc1(z))\n",
        "        x_rec = self.fc2(out)\n",
        "        return x_rec\n"
      ],
      "metadata": {
        "id": "skdYxQQVE107"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "batch_size = 4096\n",
        "X_train, X_test_normal = train_test_split(X_normal, test_size=0.01, random_state=42)\n",
        "X_normal_tensor = torch.tensor(X_normal.values.astype(np.float32), dtype=torch.float32)\n",
        "train_loader = torch.utils.data.DataLoader(X_normal_tensor, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Y6T3ydDdei3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "# Training Loop\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = X_normal.shape[1]\n",
        "learning_rate_gen = 1e-4   # Generator (Encoder)\n",
        "learning_rate_disc_xx = 1e-5  # Discriminator (lower than encoder)\n",
        "learning_rate_enc = 1e-4\n",
        "learning_rate_disc_xz = 1e-7\n",
        "learning_rate_disc_zz = 1e-10\n",
        "latent_dim = 128\n",
        "x_dim = input_dim\n",
        "num_epochs = 100\n",
        "log_interval = 100\n",
        "\n",
        "# Loss function\n",
        "criterion = F.binary_cross_entropy_with_logits\n",
        "criterion_d = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "# Move models to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize networks (example constructors; adapt as needed)\n",
        "encoder = Encoder(x_dim, latent_dim).to(device)\n",
        "generator = Generator(latent_dim, x_dim).to(device)\n",
        "discriminator_xz = DiscriminatorXZ(x_dim, latent_dim).to(device)\n",
        "discriminator_xx = DiscriminatorXX(x_dim).to(device)\n",
        "discriminator_zz = DiscriminatorZZ(latent_dim).to(device)\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "optimizer_D_xz = optim.Adam(discriminator_xz.parameters(), lr=learning_rate_disc_xz, betas=(0.5, 0.999))\n",
        "optimizer_D_xx = optim.Adam(discriminator_xx.parameters(), lr=learning_rate_disc_xx, betas=(0.5, 0.999))\n",
        "optimizer_D_zz = optim.Adam(discriminator_zz.parameters(), lr=learning_rate_disc_zz, betas=(0.5, 0.999))\n",
        "\n",
        "# Separate optimizers for generator and encoder\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_gen, betas=(0.5, 0.999))\n",
        "optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate_enc, betas=(0.5, 0.999))\n",
        "\n",
        "# Define a clipping value (adjust as needed)\n",
        "discriminator_update_interval=10\n",
        "# recon_criterion = torch.nn.L1Loss()\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    generator.train()\n",
        "    discriminator_xz.train()\n",
        "    discriminator_xx.train()\n",
        "    discriminator_zz.train()\n",
        "\n",
        "    # For logging losses per epoch (average per sample)\n",
        "    total_loss_D_xz = 0.0\n",
        "    total_loss_D_xx = 0.0\n",
        "    total_loss_D_zz = 0.0\n",
        "    total_loss_G = 0.0\n",
        "    total_loss_E = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, real_x in enumerate(train_loader):\n",
        "        # real_x = real_x.to(device)\n",
        "        real_x = real_x.type(torch.FloatTensor).to(device)\n",
        "        # real_x.requires_grad = True\n",
        "        batch_size = real_x.size(0)\n",
        "        current_batch_size = batch_size\n",
        "        n_batches += 1\n",
        "\n",
        "        # Define labels (adapted: real=0, fake=1)\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "        if i % discriminator_update_interval == 0:\n",
        "            # ============================\n",
        "            # 1. Update Discriminator_xz\n",
        "            # ============================\n",
        "            optimizer_D_xz.zero_grad()\n",
        "\n",
        "            # Real pairs: (x, encoder(x))\n",
        "            z_enc = encoder(real_x)\n",
        "            logits_real_xz, _ = discriminator_xz(real_x, z_enc)\n",
        "            loss_real_xz = criterion_d(logits_real_xz, real_labels)\n",
        "\n",
        "            # Fake pairs: (generator(z_noise), z_noise)\n",
        "            z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "            x_fake = generator(z_noise).detach()\n",
        "            logits_fake_xz, _ = discriminator_xz(x_fake, z_noise)\n",
        "            loss_fake_xz = criterion_d(logits_fake_xz, fake_labels)\n",
        "\n",
        "            loss_D_xz = (loss_real_xz + loss_fake_xz) #(loss_real_xz + loss_fake_xz) / 2\n",
        "            loss_D_xz.backward()\n",
        "            optimizer_D_xz.step()\n",
        "\n",
        "            # ============================\n",
        "            # 2. Update Discriminator_xx\n",
        "            # ============================\n",
        "            optimizer_D_xx.zero_grad()\n",
        "\n",
        "            # Real pairs: (x, x)\n",
        "            logits_real_xx, _ = discriminator_xx(real_x, real_x)\n",
        "            loss_real_xx = criterion_d(logits_real_xx, real_labels)\n",
        "\n",
        "            # Fake pairs: (x, generator(encoder(x)))\n",
        "            x_rec = generator(encoder(real_x)).detach()\n",
        "            logits_fake_xx, _ = discriminator_xx(real_x, x_rec)\n",
        "            loss_fake_xx = criterion_d(logits_fake_xx, fake_labels)\n",
        "\n",
        "            loss_D_xx = (loss_real_xx + loss_fake_xx)  #(loss_real_xx + loss_fake_xx) / 2\n",
        "            loss_D_xx.backward()\n",
        "            optimizer_D_xx.step()\n",
        "\n",
        "            # ============================\n",
        "            # 3. Update Discriminator_zz\n",
        "            # ============================\n",
        "            optimizer_D_zz.zero_grad()\n",
        "\n",
        "            # Real pairs: (z, z) where z is sampled from the prior (noise)\n",
        "            z_prior = torch.randn(batch_size, latent_dim, device=device)\n",
        "            logits_real_zz, _ = discriminator_zz(z_prior, z_prior)\n",
        "            loss_real_zz = criterion_d(logits_real_zz, real_labels)\n",
        "\n",
        "            # Fake pairs: (z, encoder(generator(z)))\n",
        "            x_fake = generator(z_prior)\n",
        "            z_rec = encoder(x_fake).detach()\n",
        "            logits_fake_zz, _ = discriminator_zz(z_prior, z_rec)\n",
        "            loss_fake_zz = criterion_d(logits_fake_zz, fake_labels)\n",
        "\n",
        "            loss_D_zz = (loss_real_zz + loss_fake_zz)#(loss_real_zz + loss_fake_zz) / 2\n",
        "            loss_D_zz.backward()\n",
        "            optimizer_D_zz.step()\n",
        "\n",
        "        #### GEN Code\n",
        "        # Assume the following tensors are already computed:\n",
        "        # l_generator: logits from some branch of the generator (used for adversarial loss)\n",
        "        # l_encoder: logits from some branch of the encoder\n",
        "        # x_logit_real, x_logit_fake: discriminator outputs (logits) for the x branch (real x and reconstructed x)\n",
        "        # z_logit_real, z_logit_fake: discriminator outputs (logits) for the z branch (real z and reconstructed z)\n",
        "        # allow_zz: Boolean flag indicating whether to include the z branch in cycle consistency loss\n",
        "        optimizer_G.zero_grad()\n",
        "        optimizer_E.zero_grad()\n",
        "\n",
        "        # z_noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "        # x_fake = generator(z_noise)\n",
        "        # l_generator, _ = discriminator_xz(x_fake, z_noise)\n",
        "        # Adversarial loss for the generator: now we use real label = 0 (instead of 1)\n",
        "        gen_loss_xz = criterion_d(logits_fake_xz, torch.ones_like(logits_fake_xz))\n",
        "\n",
        "        # For the encoder, we use the loss computed from discriminator_xz.\n",
        "        # l_encoder, _ = discriminator_xz(real_x, encoder(real_x))\n",
        "        # Adversarial loss for the generator: now we use real label = 0 (instead of 1)\n",
        "        enc_loss_xz = criterion_d(logits_real_xz, torch.zeros_like(logits_real_xz))\n",
        "\n",
        "        # Cycle consistency loss for the x branch:\n",
        "        # For real x (should be classified as real, i.e., 0)\n",
        "\n",
        "        # x_real_dis, _ = discriminator_xx(real_x, real_x)\n",
        "\n",
        "        # # Fake pairs: (x, generator(encoder(x)))\n",
        "        # x_rec = generator(encoder(real_x))\n",
        "        # x_fake_dis, _ = discriminator_xx(real_x, x_rec)\n",
        "\n",
        "\n",
        "        x_real_gen = criterion_d(logits_real_xx, torch.zeros_like(logits_real_xx))\n",
        "\n",
        "        # For fake (reconstructed) x (should be classified as fake, i.e., 1)\n",
        "        x_fake_gen = criterion_d(logits_fake_xx, torch.ones_like(x_fake_dis))\n",
        "\n",
        "        cost_x = x_real_gen + x_fake_gen\n",
        "\n",
        "        # # Cycle consistency loss for the z branch:\n",
        "\n",
        "        # # Real pairs: (z, z) where z is sampled from the prior (noise)\n",
        "        # z_prior = torch.randn(batch_size, latent_dim, device=device)\n",
        "        # z_real_dis, _ = discriminator_zz(z_prior, z_prior)\n",
        "\n",
        "        # # Fake pairs: (z, encoder(generator(z)))\n",
        "        # x_fake = generator(z_prior)\n",
        "        # z_rec = encoder(x_fake)\n",
        "        # z_fake_dis, _ = discriminator_zz(z_prior, z_rec)\n",
        "\n",
        "\n",
        "        z_real_gen = criterion_d(logits_real_zz, torch.zeros_like(logits_real_zz))\n",
        "        z_fake_gen = criterion_d(logits_fake_zz, torch.ones_like(logits_fake_zz))\n",
        "        cost_z = z_real_gen + z_fake_gen\n",
        "        # --- Cycle consistency loss (Reconstruction Loss) ---\n",
        "        # Compute reconstruction x_rec = G(E(x)) and compare to real x.\n",
        "        # x_rec = generator(encoder(real_x))\n",
        "        # recon_loss = recon_criterion(x_rec, real_x)\n",
        "        # cycle_consistency_loss = recon_loss\n",
        "\n",
        "        # Total cycle-consistency loss: include z branch if allowed\n",
        "        cycle_consistency_loss = cost_x + cost_z\n",
        "        # cycle_consistency_loss = cost_x\n",
        "\n",
        "        # Final losses:\n",
        "        loss_generator = gen_loss_xz + cycle_consistency_loss\n",
        "        # loss_generator.backward(retain_graph=True)\n",
        "        # optimizer_G.step()\n",
        "\n",
        "        loss_encoder = enc_loss_xz + cycle_consistency_loss\n",
        "        # loss_encoder.backward()\n",
        "        # optimizer_E.step()\n",
        "\n",
        "        lambda_cycle = 1.0  # You can increase this value to enforce stronger cycle consistency.\n",
        "\n",
        "        # Final losses for the generator and encoder:\n",
        "        loss_generator = gen_loss_xz + lambda_cycle * cycle_consistency_loss\n",
        "\n",
        "        loss_encoder = enc_loss_xz + lambda_cycle * cycle_consistency_loss\n",
        "\n",
        "        total_loss = loss_generator + loss_encoder\n",
        "        total_loss.backward()\n",
        "        optimizer_G.step()\n",
        "        optimizer_E.step()\n",
        "\n",
        "\n",
        "        # Log per-iteration (average loss per sample)\n",
        "        total_loss_D_xz += loss_D_xz.item() / current_batch_size if i % discriminator_update_interval == 0 else 0\n",
        "        total_loss_D_xx += loss_D_xx.item() / current_batch_size if i % discriminator_update_interval == 0 else 0\n",
        "        total_loss_D_zz += loss_D_zz.item() / current_batch_size if i % discriminator_update_interval == 0 else 0\n",
        "        total_loss_G    += loss_generator.item() / current_batch_size\n",
        "        total_loss_E    += loss_encoder.item() / current_batch_size\n",
        "\n",
        "\n",
        "    # End of epoch: compute and log average losses for discriminators (if updated)\n",
        "    num_disc_updates = (n_batches // discriminator_update_interval) or 1\n",
        "    avg_loss_D_xz = total_loss_D_xz / num_disc_updates\n",
        "    avg_loss_D_xx = total_loss_D_xx / num_disc_updates\n",
        "    avg_loss_D_zz = total_loss_D_zz / num_disc_updates\n",
        "    avg_loss_G = total_loss_G / n_batches\n",
        "    avg_loss_E = total_loss_E / n_batches\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Summary:\")\n",
        "    print(f\"  Average Loss_D_xz: {avg_loss_D_xz:.4f}  Average Loss_D_xx: {avg_loss_D_xx:.4f} Average Loss_D_zz: {avg_loss_D_zz:.4f}\")\n",
        "    # print(f\"  Average Loss_D_xz: {avg_loss_D_xz:.4f}  Average Loss_D_xx: {avg_loss_D_xx:.4f}\")\n",
        "    print(f\"  Average Loss_G: {avg_loss_G:.4f}, Average Loss_E: {avg_loss_E:.4f}\")\n",
        "\n",
        "print(\"Training Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9NUBb_iK3e",
        "outputId": "fc29f91e-a806-4066-c0f2-b5131d2f9f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Summary:\n",
            "  Average Loss_D_xz: 0.2830  Average Loss_D_xx: 0.4776 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 4.3895, Average Loss_E: 4.6364\n",
            "Epoch [2/100] Summary:\n",
            "  Average Loss_D_xz: 0.0302  Average Loss_D_xx: 0.2840 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 3.0203, Average Loss_E: 3.0456\n",
            "Epoch [3/100] Summary:\n",
            "  Average Loss_D_xz: 0.0095  Average Loss_D_xx: 0.2252 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 2.4706, Average Loss_E: 2.4786\n",
            "Epoch [4/100] Summary:\n",
            "  Average Loss_D_xz: 0.0048  Average Loss_D_xx: 0.1947 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 2.0496, Average Loss_E: 2.0529\n",
            "Epoch [5/100] Summary:\n",
            "  Average Loss_D_xz: 0.0025  Average Loss_D_xx: 0.1572 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 1.7116, Average Loss_E: 1.7138\n",
            "Epoch [6/100] Summary:\n",
            "  Average Loss_D_xz: 0.0023  Average Loss_D_xx: 0.1348 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 1.4390, Average Loss_E: 1.4403\n",
            "Epoch [7/100] Summary:\n",
            "  Average Loss_D_xz: 0.0016  Average Loss_D_xx: 0.1173 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 1.2467, Average Loss_E: 1.2477\n",
            "Epoch [8/100] Summary:\n",
            "  Average Loss_D_xz: 0.0011  Average Loss_D_xx: 0.0973 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 1.0769, Average Loss_E: 1.0778\n",
            "Epoch [9/100] Summary:\n",
            "  Average Loss_D_xz: 0.0012  Average Loss_D_xx: 0.0858 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.9046, Average Loss_E: 0.9052\n",
            "Epoch [10/100] Summary:\n",
            "  Average Loss_D_xz: 0.0011  Average Loss_D_xx: 0.0752 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.7954, Average Loss_E: 0.7958\n",
            "Epoch [11/100] Summary:\n",
            "  Average Loss_D_xz: 0.0011  Average Loss_D_xx: 0.0674 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.7032, Average Loss_E: 0.7035\n",
            "Epoch [12/100] Summary:\n",
            "  Average Loss_D_xz: 0.0007  Average Loss_D_xx: 0.0590 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.6331, Average Loss_E: 0.6334\n",
            "Epoch [13/100] Summary:\n",
            "  Average Loss_D_xz: 0.0007  Average Loss_D_xx: 0.0521 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.5590, Average Loss_E: 0.5592\n",
            "Epoch [14/100] Summary:\n",
            "  Average Loss_D_xz: 0.0005  Average Loss_D_xx: 0.0460 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.4996, Average Loss_E: 0.4998\n",
            "Epoch [15/100] Summary:\n",
            "  Average Loss_D_xz: 0.0005  Average Loss_D_xx: 0.0407 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.4544, Average Loss_E: 0.4546\n",
            "Epoch [16/100] Summary:\n",
            "  Average Loss_D_xz: 0.0006  Average Loss_D_xx: 0.0380 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.4154, Average Loss_E: 0.4155\n",
            "Epoch [17/100] Summary:\n",
            "  Average Loss_D_xz: 0.0005  Average Loss_D_xx: 0.0339 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.3638, Average Loss_E: 0.3638\n",
            "Epoch [18/100] Summary:\n",
            "  Average Loss_D_xz: 0.0005  Average Loss_D_xx: 0.0310 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.3384, Average Loss_E: 0.3384\n",
            "Epoch [19/100] Summary:\n",
            "  Average Loss_D_xz: 0.0005  Average Loss_D_xx: 0.0281 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.3062, Average Loss_E: 0.3062\n",
            "Epoch [20/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0270 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.2898, Average Loss_E: 0.2898\n",
            "Epoch [21/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0235 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.2500, Average Loss_E: 0.2500\n",
            "Epoch [22/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0214 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.2462, Average Loss_E: 0.2461\n",
            "Epoch [23/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0208 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.2176, Average Loss_E: 0.2176\n",
            "Epoch [24/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0190 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.2078, Average Loss_E: 0.2078\n",
            "Epoch [25/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0168 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1856, Average Loss_E: 0.1855\n",
            "Epoch [26/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0163 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1813, Average Loss_E: 0.1813\n",
            "Epoch [27/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0151 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1683, Average Loss_E: 0.1683\n",
            "Epoch [28/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0146 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1547, Average Loss_E: 0.1547\n",
            "Epoch [29/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0138 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1497, Average Loss_E: 0.1496\n",
            "Epoch [30/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0128 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1395, Average Loss_E: 0.1394\n",
            "Epoch [31/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0121 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1309, Average Loss_E: 0.1309\n",
            "Epoch [32/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0116 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1260, Average Loss_E: 0.1259\n",
            "Epoch [33/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0106 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1192, Average Loss_E: 0.1191\n",
            "Epoch [34/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0103 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1146, Average Loss_E: 0.1145\n",
            "Epoch [35/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0103 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1080, Average Loss_E: 0.1079\n",
            "Epoch [36/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0091 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.1154, Average Loss_E: 0.1153\n",
            "Epoch [37/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0093 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0957, Average Loss_E: 0.0957\n",
            "Epoch [38/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0078 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0908, Average Loss_E: 0.0907\n",
            "Epoch [39/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0082 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0948, Average Loss_E: 0.0947\n",
            "Epoch [40/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0073 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0829, Average Loss_E: 0.0828\n",
            "Epoch [41/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0073 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0808, Average Loss_E: 0.0807\n",
            "Epoch [42/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0076 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0760, Average Loss_E: 0.0759\n",
            "Epoch [43/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0070 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0783, Average Loss_E: 0.0782\n",
            "Epoch [44/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0056 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0746, Average Loss_E: 0.0745\n",
            "Epoch [45/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0067 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0751, Average Loss_E: 0.0750\n",
            "Epoch [46/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0056 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0666, Average Loss_E: 0.0665\n",
            "Epoch [47/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0052 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0612, Average Loss_E: 0.0611\n",
            "Epoch [48/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0054 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0633, Average Loss_E: 0.0632\n",
            "Epoch [49/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0052 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0599, Average Loss_E: 0.0598\n",
            "Epoch [50/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0051 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0566, Average Loss_E: 0.0565\n",
            "Epoch [51/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0048 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0567, Average Loss_E: 0.0566\n",
            "Epoch [52/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0044 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0539, Average Loss_E: 0.0538\n",
            "Epoch [53/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0046 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0521, Average Loss_E: 0.0520\n",
            "Epoch [54/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0043 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0484, Average Loss_E: 0.0483\n",
            "Epoch [55/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0044 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0457, Average Loss_E: 0.0456\n",
            "Epoch [56/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0038 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0441, Average Loss_E: 0.0440\n",
            "Epoch [57/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0041 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0452, Average Loss_E: 0.0451\n",
            "Epoch [58/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0037 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0458, Average Loss_E: 0.0457\n",
            "Epoch [59/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0035 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0434, Average Loss_E: 0.0433\n",
            "Epoch [60/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0037 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0424, Average Loss_E: 0.0422\n",
            "Epoch [61/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0034 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0390, Average Loss_E: 0.0389\n",
            "Epoch [62/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0033 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0435, Average Loss_E: 0.0434\n",
            "Epoch [63/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0035 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0407, Average Loss_E: 0.0406\n",
            "Epoch [64/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0028 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0371, Average Loss_E: 0.0370\n",
            "Epoch [65/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0032 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0350, Average Loss_E: 0.0349\n",
            "Epoch [66/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0028 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0347, Average Loss_E: 0.0346\n",
            "Epoch [67/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0028 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0315, Average Loss_E: 0.0314\n",
            "Epoch [68/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0028 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0324, Average Loss_E: 0.0323\n",
            "Epoch [69/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0027 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0335, Average Loss_E: 0.0334\n",
            "Epoch [70/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0026 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0354, Average Loss_E: 0.0352\n",
            "Epoch [71/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0028 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0300, Average Loss_E: 0.0299\n",
            "Epoch [72/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0026 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0312, Average Loss_E: 0.0311\n",
            "Epoch [73/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0023 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0298, Average Loss_E: 0.0296\n",
            "Epoch [74/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0023 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0286, Average Loss_E: 0.0285\n",
            "Epoch [75/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0022 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0267, Average Loss_E: 0.0266\n",
            "Epoch [76/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0018 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0291, Average Loss_E: 0.0290\n",
            "Epoch [77/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0020 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0268, Average Loss_E: 0.0267\n",
            "Epoch [78/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0020 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0267, Average Loss_E: 0.0266\n",
            "Epoch [79/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0021 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0261, Average Loss_E: 0.0260\n",
            "Epoch [80/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0019 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0233, Average Loss_E: 0.0233\n",
            "Epoch [81/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0019 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0254, Average Loss_E: 0.0253\n",
            "Epoch [82/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0020 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0223, Average Loss_E: 0.0222\n",
            "Epoch [83/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0017 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0255, Average Loss_E: 0.0254\n",
            "Epoch [84/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0018 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0243, Average Loss_E: 0.0242\n",
            "Epoch [85/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0019 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0276, Average Loss_E: 0.0275\n",
            "Epoch [86/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0017 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0223, Average Loss_E: 0.0222\n",
            "Epoch [87/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0017 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0205, Average Loss_E: 0.0204\n",
            "Epoch [88/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0015 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0208, Average Loss_E: 0.0208\n",
            "Epoch [89/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0018 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0210, Average Loss_E: 0.0209\n",
            "Epoch [90/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0015 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0222, Average Loss_E: 0.0221\n",
            "Epoch [91/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0014 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0199, Average Loss_E: 0.0198\n",
            "Epoch [92/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0013 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0215, Average Loss_E: 0.0213\n",
            "Epoch [93/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0015 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0202, Average Loss_E: 0.0201\n",
            "Epoch [94/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0015 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0207, Average Loss_E: 0.0206\n",
            "Epoch [95/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0013 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0217, Average Loss_E: 0.0216\n",
            "Epoch [96/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0013 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0175, Average Loss_E: 0.0175\n",
            "Epoch [97/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0012 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0206, Average Loss_E: 0.0205\n",
            "Epoch [98/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0012 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0177, Average Loss_E: 0.0176\n",
            "Epoch [99/100] Summary:\n",
            "  Average Loss_D_xz: 0.0003  Average Loss_D_xx: 0.0014 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0185, Average Loss_E: 0.0184\n",
            "Epoch [100/100] Summary:\n",
            "  Average Loss_D_xz: 0.0004  Average Loss_D_xx: 0.0012 Average Loss_D_zz: 0.0004\n",
            "  Average Loss_G: 0.0188, Average Loss_E: 0.0187\n",
            "Training Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "encoder.eval()\n",
        "generator.eval()\n",
        "discriminator_xx.eval()\n",
        "\n",
        "anomaly_scores = []\n",
        "y_true = []\n",
        "inference_times = []\n",
        "\n",
        "# X_test_normal.shape: (5699, 462)\n",
        "# X_anomaly.shape: (20663, 462)\n",
        "\n",
        "\n",
        "# Create the test dataset and labels\n",
        "\n",
        "\n",
        "X_test = pd.concat([X_normal, X_anomaly])\n",
        "y_test = np.concatenate([np.zeros(X_normal.shape[0], dtype=int), np.ones(X_anomaly.shape[0], dtype=int)])\n",
        "\n",
        "print(f\"X_test.shape: {X_test.shape}\")\n",
        "print(f\"y_test.shape: {y_test.shape}\")\n",
        "\n",
        "# Convert to tensors\n",
        "test_x_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "test_y_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create a TensorDataset from the tensors.\n",
        "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
        "\n",
        "# Create a DataLoader. The batch_size can be adjusted as needed.\n",
        "batch_size = 4096 # Example batch size; adjust as needed.\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def compute_anomaly_score_combined(cnn_codes_orig, cnn_codes_rec, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Computes the anomaly score as a weighted combination of the L1 and L2 norms.\n",
        "    alpha: weight for L1 loss (between 0 and 1). (1-alpha) is the weight for L2 loss.\n",
        "    \"\"\"\n",
        "    l1_score = torch.mean(torch.abs(cnn_codes_orig - cnn_codes_rec), dim=1)\n",
        "    l2_score = torch.norm(cnn_codes_orig - cnn_codes_rec, p=2, dim=1)\n",
        "    # Combine the scores.\n",
        "    combined_score = alpha * l1_score + (1 - alpha) * l2_score\n",
        "    return combined_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_batch, labels in test_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1. Get CNN codes for original samples from Dxx.\n",
        "        #    Here, we assume model_Dxx(x) returns (logits, cnn_code)\n",
        "        _, cnn_codes_orig = discriminator_xx(x_batch,x_batch)\n",
        "\n",
        "        # 2. Compute reconstruction x_rec = G(E(x))\n",
        "        z = encoder(x_batch)\n",
        "        x_rec = generator(z)\n",
        "\n",
        "        # 3. Get CNN codes for reconstructed samples.\n",
        "        _, cnn_codes_rec = discriminator_xx(x_rec,x_rec)\n",
        "\n",
        "        # 4. Compute the L1 reconstruction error in the feature space (per sample).\n",
        "        #    Using mean absolute error (you could also use sum).\n",
        "        # batch_scores = torch.mean(torch.abs(cnn_codes_orig - cnn_codes_rec), dim=1)\n",
        "        # batch_scores = torch.norm(cnn_codes_orig - cnn_codes_rec, p=2, dim=1)\n",
        "        batch_scores = compute_anomaly_score_combined(cnn_codes_orig,cnn_codes_rec,alpha=1)\n",
        "\n",
        "        anomaly_scores.extend(batch_scores.cpu().numpy().tolist())\n",
        "        y_true.extend(labels.cpu().numpy().tolist())\n",
        "        # Record and store the inference time for this batch.\n",
        "        batch_inference_time = time.time() - start_time\n",
        "        inference_times.append(batch_inference_time)\n",
        "\n",
        "print(\"y_true: {}\".format(y_true[:5]))\n",
        "print(\"anomaly_scores: {}\".format(anomaly_scores[:5])) # Access the first element of the desired rows using slicing\n",
        "\n",
        "# Compute AUROC using the anomaly scores.\n",
        "auroc = roc_auc_score(y_true, anomaly_scores)\n",
        "print(\"AUROC (based on Dxx feature reconstruction error): {:.4f}\".format(auroc))\n",
        "\n",
        "# Calculate average inference time over all batches.\n",
        "mean_inference_time = np.mean(inference_times)\n",
        "print(\"Mean inference time per batch: {:.4f} sec\".format(mean_inference_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assume y_true is a list/array of true labels (0 for normal, 1 for anomaly)\n",
        "# and anomaly_scores is a list/array of your model's anomaly scores\n",
        "average_precision = average_precision_score(y_true, anomaly_scores)\n",
        "print(\"Average Precision (AUPRC): {:.4f}\".format(average_precision))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMyxQkO3QNUc",
        "outputId": "d7bafe6c-e999-492f-bb1a-1e9ace76011e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test.shape: (590540, 462)\n",
            "y_test.shape: (590540,)\n",
            "y_true: [0, 0, 0, 0, 0]\n",
            "anomaly_scores: [8867.166015625, 2436.90087890625, 13630.40234375, 12370.791015625, 6786.87646484375]\n",
            "AUROC (based on Dxx feature reconstruction error): 0.5173\n",
            "Mean inference time per batch: 0.0045 sec\n",
            "Average Precision (AUPRC): 0.0364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "encoder.eval()\n",
        "generator.eval()\n",
        "discriminator_xx.eval()\n",
        "\n",
        "anomaly_scores = []\n",
        "y_true = []\n",
        "inference_times = []\n",
        "\n",
        "print(f\"X_test_normal.shape: {X_test_normal.shape}\")\n",
        "print(f\"X_anomaly.shape: {X_anomaly.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "# Randomly sample 5 samples each from X_test_normal and X_anomaly\n",
        "num_samples = 1000\n",
        "num_samples_an = 100\n",
        "X_test_normal_sampled = X_test_normal.sample(n=num_samples, random_state=42) # Replace X_test_normal with the variable holding the normal test data\n",
        "X_anomaly_sampled = X_anomaly.sample(n=num_samples_an, random_state=42)\n",
        "print(f\"X_test_normal_sampled.shape: {X_test_normal_sampled.shape}\")\n",
        "print(f\"X_anomaly_sampled.shape: {X_anomaly_sampled.shape}\")\n",
        "\n",
        "# Create the test dataset and labels\n",
        "X_test = pd.concat([X_test_normal_sampled, X_anomaly_sampled])\n",
        "y_test = np.concatenate([np.zeros(num_samples, dtype=int), np.ones(num_samples_an, dtype=int)])\n",
        "\n",
        "# # Create an index array\n",
        "indices = np.arange(X_test.shape[0])\n",
        "\n",
        "# Shuffle the index array\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Use the shuffled indices to reorder X_test and y_test\n",
        "X_test_shuffled = X_test.iloc[indices]\n",
        "y_test_shuffled = y_test[indices]\n",
        "\n",
        "\n",
        "# Convert to tensors\n",
        "test_x_tensor = torch.tensor(X_test_shuffled.values, dtype=torch.float32).to(device)\n",
        "test_y_tensor = torch.tensor(y_test_shuffled, dtype=torch.long).to(device)\n",
        "\n",
        "# Create a TensorDataset from the tensors.\n",
        "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
        "\n",
        "# Create a DataLoader. The batch_size can be adjusted as needed.\n",
        "batch_size = 32 # Example batch size; adjust as needed.\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def compute_anomaly_score_combined(cnn_codes_orig, cnn_codes_rec, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Computes the anomaly score as a weighted combination of the L1 and L2 norms.\n",
        "    alpha: weight for L1 loss (between 0 and 1). (1-alpha) is the weight for L2 loss.\n",
        "    \"\"\"\n",
        "    l1_score = torch.mean(torch.abs(cnn_codes_orig - cnn_codes_rec), dim=1)\n",
        "    l2_score = torch.norm(cnn_codes_orig - cnn_codes_rec, p=2, dim=1)\n",
        "    # Combine the scores.\n",
        "    combined_score = alpha * l1_score + (1 - alpha) * l2_score\n",
        "    return combined_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_batch, labels in test_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1. Get CNN codes for original samples from Dxx.\n",
        "        #    Here, we assume model_Dxx(x) returns (logits, cnn_code)\n",
        "        _, cnn_codes_orig = discriminator_xx(x_batch,x_batch)\n",
        "\n",
        "        # 2. Compute reconstruction x_rec = G(E(x))\n",
        "        z = encoder(x_batch)\n",
        "        x_rec = generator(z)\n",
        "\n",
        "        # 3. Get CNN codes for reconstructed samples.\n",
        "        _, cnn_codes_rec = discriminator_xx(x_rec,x_rec)\n",
        "\n",
        "        # 4. Compute the L1 reconstruction error in the feature space (per sample).\n",
        "        #    Using mean absolute error (you could also use sum).\n",
        "        # batch_scores = torch.mean(torch.abs(cnn_codes_orig - cnn_codes_rec), dim=1)\n",
        "        # batch_scores = torch.norm(cnn_codes_orig - cnn_codes_rec, p=2, dim=1)\n",
        "        batch_scores = compute_anomaly_score_combined(cnn_codes_orig,cnn_codes_rec,alpha=0.5)\n",
        "\n",
        "        anomaly_scores.extend(batch_scores.cpu().numpy().tolist())\n",
        "        y_true.extend(labels.cpu().numpy().tolist())\n",
        "        # Record and store the inference time for this batch.\n",
        "        batch_inference_time = time.time() - start_time\n",
        "        inference_times.append(batch_inference_time)\n",
        "\n",
        "print(\"y_true: {}\".format(y_true[:5]))\n",
        "print(\"anomaly_scores: {}\".format(anomaly_scores[:5])) # Access the first element of the desired rows using slicing\n",
        "\n",
        "# Compute AUROC using the anomaly scores.\n",
        "auroc = roc_auc_score(y_true, anomaly_scores)\n",
        "print(\"AUROC (based on Dxx feature reconstruction error): {:.4f}\".format(auroc))\n",
        "\n",
        "# Calculate average inference time over all batches.\n",
        "mean_inference_time = np.mean(inference_times)\n",
        "print(\"Mean inference time per batch: {:.4f} sec\".format(mean_inference_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assume y_true is a list/array of true labels (0 for normal, 1 for anomaly)\n",
        "# and anomaly_scores is a list/array of your model's anomaly scores\n",
        "average_precision = average_precision_score(y_true, anomaly_scores)\n",
        "print(\"Average Precision (AUPRC): {:.4f}\".format(average_precision))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSxrV5Y0wYGd",
        "outputId": "ccf6ece0-1f98-4129-f4fa-32c7a85113d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test_normal.shape: (5699, 462)\n",
            "X_anomaly.shape: (20663, 462)\n",
            "X_test_normal_sampled.shape: (1000, 462)\n",
            "X_anomaly_sampled.shape: (100, 462)\n",
            "y_true: [0, 0, 0, 0, 0]\n",
            "anomaly_scores: [107399.1796875, 137192.109375, 74140.3125, 109456.25, 127363.40625]\n",
            "AUROC (based on Dxx feature reconstruction error): 0.4988\n",
            "Mean inference time per batch: 0.0015 sec\n",
            "Average Precision (AUPRC): 0.0930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Suppose these are defined from your ALAD model inference:\n",
        "# y_true: true labels, e.g. np.array([...])\n",
        "# anomaly_scores: continuous anomaly scores, e.g. np.array([...])\n",
        "\n",
        "# Convert lists to numpy arrays if necessary.\n",
        "y_true = np.array(y_true)\n",
        "anomaly_scores = np.array(anomaly_scores)\n",
        "\n",
        "# Option 1: Determine threshold using the 95th percentile of normal samples.\n",
        "# (Assumes that normal samples are labeled 0.)\n",
        "normal_scores = anomaly_scores[y_true == 0]\n",
        "threshold = np.percentile(normal_scores, 90)\n",
        "print(\"Threshold based on 95th percentile of normal samples:\", threshold)\n",
        "\n",
        "# Option 2: Or set a manual threshold (uncomment below if needed).\n",
        "# threshold = 0.5\n",
        "\n",
        "# Generate binary predictions: predict fraud (1) if score > threshold, else normal (0)\n",
        "y_pred = (anomaly_scores < threshold).astype(int)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"Precision: {:.4f}\".format(precision))\n",
        "print(\"Recall: {:.4f}\".format(recall))\n",
        "print(\"F1 Score: {:.4f}\".format(f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK98vlyz5aaU",
        "outputId": "b0a0043b-b50f-4554-fced-aab47c037c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold based on 95th percentile of normal samples: 165103.11718750003\n",
            "Precision: 0.0900\n",
            "Recall: 0.8900\n",
            "F1 Score: 0.1635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assume y_true and anomaly_scores are NumPy arrays\n",
        "# where y_true==0 for normal and y_true==1 for anomalies.\n",
        "\n",
        "\n",
        "normal_scores = anomaly_scores[y_true == 0]\n",
        "anomaly_scores_only = anomaly_scores[y_true == 1]\n",
        "\n",
        "plt.hist(normal_scores, bins=50, alpha=0.6, label='Normal')\n",
        "plt.hist(anomaly_scores_only, bins=50, alpha=0.6, label='Anomaly')\n",
        "plt.xlabel(\"Anomaly Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Anomaly Scores\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "4PfQB52wCfsL",
        "outputId": "2a48a753-baa5-4269-a21f-ea25cf74823b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS85JREFUeJzt3XdUFNf/PvBnpXcEpYkCoggK9ojYoyTYG8YeijW22GNIYkw0CcZeYvvki2BJrLHFqIliiVGMQuwaIkZFpcUCiEoR7u8PD/NzpS+LuwPP65w9x71zZ+Y9O8A+zsydUQghBIiIiIhkqJqmCyAiIiJSFYMMERERyRaDDBEREckWgwwRERHJFoMMERERyRaDDBEREckWgwwRERHJFoMMERERyRaDDBEREckWgwxVSV988QUUCsUbWVenTp3QqVMn6f3x48ehUCiwc+fON7L+oKAgODs7v5F1qSojIwOjRo2CnZ0dFAoFpkyZoumSNO5N/owSyRmDDMleREQEFAqF9DI0NISDgwP8/PywYsUKPHnyRC3rSUhIwBdffIELFy6oZXnqpM21lcY333yDiIgIjBs3Dps2bcL7779f4jy5ublwcHCAQqHAwYMH30CV8paXl4eNGzfC29sbVlZWMDMzg5ubGwICAnDmzBlNl0ekMl1NF0CkLnPnzoWLiwtycnKQlJSE48ePY8qUKViyZAn27duHxo0bS30/++wzfPzxx2VafkJCAr788ks4OzujadOmpZ7vt99+K9N6VFFcbd9//z3y8vIqvIbyOHr0KFq3bo05c+aUaZ7ExEQ4Ozvjhx9+QLdu3SqwQvn78MMPsWrVKvTp0wfDhg2Drq4uYmNjcfDgQdStWxetW7fWdIlEKmGQoUqjW7duaNmypfQ+JCQER48eRc+ePdG7d29cv34dRkZGAABdXV3o6lbsj/+zZ89gbGwMfX39Cl1PSfT09DS6/tJISUlBw4YNyzTP5s2b0bx5cwQGBuKTTz7B06dPYWJiUkEVyltycjJWr16N0aNH43//+5/StGXLluG///57Y7W8ePECeXl5Gv+9oMqDp5aoUuvcuTNmz56NO3fuYPPmzVJ7YdcfHD58GO3atYOlpSVMTU3RoEEDfPLJJwBeXtfy1ltvAQCCg4Ol01gREREAXl4H4+npiZiYGHTo0AHGxsbSvK9fI5MvNzcXn3zyCezs7GBiYoLevXvj7t27Sn2cnZ0RFBRUYN5Xl1lSbYVdI/P06VNMnz4dtWvXhoGBARo0aIBFixZBCKHUT6FQYOLEidizZw88PT1hYGCARo0a4dChQ4V/4K9JSUnByJEjYWtrC0NDQzRp0gQbNmyQpudfL3Tr1i388ssvUu23b98udrnPnz/H7t27MXjwYAwcOBDPnz/H3r17C/QLCgqCqakp7t+/j759+8LU1BQ1a9bEjBkzkJubW67PZMeOHWjYsCGMjIzg4+ODy5cvAwDWrVuHevXqwdDQEJ06dSqwLSdPnsR7772HOnXqwMDAALVr18bUqVPx/PnzYre5Y8eOaNKkSaHTGjRoAD8/vyLnvXXrFoQQaNu2bYFpCoUCNjY2Sm2pqamYOnUqnJ2dYWBgAEdHRwQEBODBgwdSn5L2LQDcvn0bCoUCixYtwrJly+Dq6goDAwNcu3YNAPD3339jwIABsLKygqGhIVq2bIl9+/YpLSMnJwdffvkl6tevD0NDQ1hbW6Ndu3Y4fPhwsZ8XVR08IkOV3vvvv49PPvkEv/32G0aPHl1on6tXr6Jnz55o3Lgx5s6dCwMDA8TFxeHUqVMAAA8PD8ydOxeff/45xowZg/bt2wMA2rRpIy3j4cOH6NatGwYPHozhw4fD1ta22Lq+/vprKBQKzJo1CykpKVi2bBl8fX1x4cIF6chRaZSmtlcJIdC7d28cO3YMI0eORNOmTfHrr79i5syZuH//PpYuXarU/48//sCuXbswfvx4mJmZYcWKFfD390d8fDysra2LrOv58+fo1KkT4uLiMHHiRLi4uGDHjh0ICgpCamoqJk+eDA8PD2zatAlTp06Fo6Mjpk+fDgCoWbNmsdu8b98+ZGRkYPDgwbCzs0OnTp3www8/YOjQoQX65ubmws/PD97e3li0aBGOHDmCxYsXw9XVFePGjVPpMzl58iT27duHCRMmAABCQ0PRs2dPfPTRR1i9ejXGjx+Px48fY8GCBRgxYgSOHj0qzbtjxw48e/YM48aNg7W1Nc6ePYuVK1fi3r172LFjR5Hb/P7772P06NG4cuUKPD09pfZz587hn3/+wWeffVbkvE5OTtK633vvPRgbGxfZNyMjA+3bt8f169cxYsQING/eHA8ePMC+fftw79491KhRo1T79lXh4eHIzMzEmDFjYGBgACsrK1y9ehVt27ZFrVq18PHHH8PExATbt29H37598dNPP6Ffv34AXv6nIzQ0FKNGjUKrVq2Qnp6O6Oho/PXXX3jnnXeK3A6qQgSRzIWHhwsA4ty5c0X2sbCwEM2aNZPez5kzR7z647906VIBQPz3339FLuPcuXMCgAgPDy8wrWPHjgKAWLt2baHTOnbsKL0/duyYACBq1aol0tPTpfbt27cLAGL58uVSm5OTkwgMDCxxmcXVFhgYKJycnKT3e/bsEQDEV199pdRvwIABQqFQiLi4OKkNgNDX11dqu3jxogAgVq5cWWBdr1q2bJkAIDZv3iy1ZWdnCx8fH2Fqaqq07U5OTqJHjx7FLu9VPXv2FG3btpXe/+9//xO6uroiJSVFqV9gYKAAIObOnavU3qxZM9GiRQvpfVk/EwMDA3Hr1i2pbd26dQKAsLOzU9qukJAQAUCp77NnzwpsT2hoqFAoFOLOnTtS2+s/o6mpqcLQ0FDMmjVLad4PP/xQmJiYiIyMjALLfVVAQIAAIKpXry769esnFi1aJK5fv16g3+effy4AiF27dhWYlpeXJ4Qo/b69deuWACDMzc0L7JsuXboILy8vkZmZqbT8Nm3aiPr160ttTZo0KdPPBlU9PLVEVYKpqWmxo5csLS0BAHv37lX5wlgDAwMEBweXun9AQADMzMyk9wMGDIC9vT0OHDig0vpL68CBA9DR0cGHH36o1D59+nQIIQqMAPL19YWrq6v0vnHjxjA3N8e///5b4nrs7OwwZMgQqU1PTw8ffvghMjIycOLECZXqf/jwIX799Vel5fr7+0OhUGD79u2FzvPBBx8ovW/fvr1S/WX9TLp06aJ0us7b21uq49V9mt/+6rpePdr29OlTPHjwAG3atIEQAufPny9yuy0sLNCnTx9s2bJFOt2Vm5uLbdu2oW/fviVeHxQeHo7vvvsOLi4u2L17N2bMmAEPDw906dIF9+/fl/r99NNPaNKkiXRE5FX5p2PLum/9/f2VjrI9evQIR48excCBA/HkyRM8ePAADx48wMOHD+Hn54cbN25INVlaWuLq1au4ceNGsdtHVReDDFUJGRkZSl8wrxs0aBDatm2LUaNGwdbWFoMHD8b27dvLFGpq1apVpgsY69evr/ReoVCgXr16JV4fUl537tyBg4NDgc/Dw8NDmv6qOnXqFFhG9erV8fjx4xLXU79+fVSrpvxnpqj1lNa2bduQk5ODZs2aIS4uDnFxcXj06BG8vb3xww8/FOhvaGhY4FTV6/WX9zOxsLAAANSuXbvQ9lfXFR8fj6CgIFhZWUnX7HTs2BEAkJaWVuy2BwQEID4+HidPngQAHDlyBMnJyaUarl6tWjVMmDABMTExePDgAfbu3Ytu3brh6NGjGDx4sNTv5s2bSqeuClPWfevi4qL0Pi4uDkIIzJ49GzVr1lR65Y9cS0lJAfByNGJqairc3Nzg5eWFmTNn4tKlSyVuL1UdvEaGKr179+4hLS0N9erVK7KPkZERfv/9dxw7dgy//PILDh06hG3btqFz58747bffoKOjU+J6ynJdS2kVdUO03NzcUtWkDkWtR7x2Eeybkh9WCrtwFXh59KNu3brS+4r4nIpaZkmfVW5uLt555x08evQIs2bNgru7O0xMTHD//n0EBQWVGJz9/Pxga2uLzZs3o0OHDti8eTPs7Ozg6+tbpvqtra3Ru3dv9O7dG506dcKJEydw584d6VoadXv9dyN/O2fMmFHkRcr5v68dOnTAzZs3sXfvXvz222/4v//7PyxduhRr167FqFGjKqRekhcekaFKb9OmTQBQ7KgO4OX/WLt06YIlS5bg2rVr+Prrr3H06FEcO3YMQNGhQlWvHyoXQiAuLk7plEX16tWRmppaYN7X/8dbltqcnJyQkJBQ4FTb33//LU1XBycnJ9y4caPAl3N51nPr1i2cPn1aGjX06mvbtm3Q19fHjz/+qFKtb+IzuXz5Mv755x8sXrwYs2bNQp8+feDr6wsHB4dSza+jo4OhQ4di586dePz4Mfbs2YMhQ4aUK6zl37IgMTERAODq6oorV64UO095921+0NTT04Ovr2+hr1ePjllZWSE4OBhbtmzB3bt30bhxY3zxxRdl2k6qvBhkqFI7evQo5s2bBxcXFwwbNqzIfo8ePSrQln9juaysLACQrkEoLFioYuPGjUpfnDt37kRiYqLSjd1cXV1x5swZZGdnS2379+8vMEy7LLV1794dubm5+O6775Taly5dCoVCobYby3Xv3h1JSUnYtm2b1PbixQusXLkSpqam0umUssg/GvPRRx9hwIABSq+BAweiY8eOhZ5eKk2tb+IzyQ8crx7NEkJg+fLlpV7G+++/j8ePH2Ps2LHIyMjA8OHDS5wnKSlJGvL8quzsbERGRqJatWrSERB/f39cvHgRu3fvLtA/v+7y7lsbGxt06tQJ69atkwLUq169r83Dhw+VppmamqJevXrS7yURTy1RpXHw4EH8/fffePHiBZKTk3H06FEcPnwYTk5O2LdvHwwNDYucd+7cufj999/Ro0cPODk5ISUlBatXr4ajoyPatWsH4GWosLS0xNq1a2FmZgYTExN4e3sXOP9fWlZWVmjXrh2Cg4ORnJyMZcuWoV69ekpDxEeNGoWdO3eia9euGDhwIG7evInNmzcrXXxb1tp69eqFt99+G59++ilu376NJk2a4LfffsPevXsxZcqUAstW1ZgxY7Bu3ToEBQUhJiYGzs7O2LlzJ06dOoVly5YVe81SUX744Qc0bdq0wLUo+Xr37o1Jkybhr7/+QvPmzUu93Df1mbi7u8PV1RUzZszA/fv3YW5ujp9++qnE641e1axZM3h6emLHjh3w8PAo1Xbeu3cPrVq1QufOndGlSxfY2dkhJSUFW7ZswcWLFzFlyhTUqFEDADBz5kzs3LkT7733HkaMGIEWLVrg0aNH2LdvH9auXYsmTZqoZd+uWrUK7dq1g5eXF0aPHo26desiOTkZUVFRuHfvHi5evAgAaNiwITp16oQWLVrAysoK0dHR2LlzJyZOnFjqz4wqOQ2NliJSm/zh1/kvfX19YWdnJ9555x2xfPlypeGw+V4f2hoZGSn69OkjHBwchL6+vnBwcBBDhgwR//zzj9J8e/fuFQ0bNhS6urpKw507duwoGjVqVGh9RQ2/3rJliwgJCRE2NjbCyMhI9OjRQ2n4bb7FixeLWrVqCQMDA9G2bVsRHR1dYJnF1fb68GshhHjy5ImYOnWqcHBwEHp6eqJ+/fpi4cKF0vDafADEhAkTCtRU1LDw1yUnJ4vg4GBRo0YNoa+vL7y8vAodIl6a4dcxMTECgJg9e3aRfW7fvi0AiKlTpwohXm67iYlJgX6v738hyveZ5A8zXrhwoVJ7/r7esWOH1Hbt2jXh6+srTE1NRY0aNcTo0aOlIe2vfjaF1ZhvwYIFAoD45ptvivwsXpWeni6WL18u/Pz8hKOjo9DT0xNmZmbCx8dHfP/99wW28eHDh2LixImiVq1aQl9fXzg6OorAwEDx4MEDqU9p9m1Rn0u+mzdvioCAAGFnZyf09PRErVq1RM+ePcXOnTulPl999ZVo1aqVsLS0FEZGRsLd3V18/fXXIjs7u1TbTpWfQggNXbFHREQqWb58OaZOnYrbt28XOqqMqCphkCEikhEhBJo0aQJra2vpQnSiqozXyBARycDTp0+xb98+HDt2DJcvXy702VJEVRGPyBARycDt27fh4uICS0tLjB8/Hl9//bWmSyLSCgwyREREJFu8jwwRERHJFoMMERERyValv9g3Ly8PCQkJMDMzU/st5omIiKhiCCHw5MkTODg4FHhA6asqfZBJSEgo8i6gREREpN3u3r0LR0fHIqdX+iCTf6vsu3fvwtzcXMPVEBERUWmkp6ejdu3aJT7yotIHmfzTSebm5gwyREREMlPSZSG82JeIiIhki0GGiIiIZItBhoiIiGSr0l8jQ0REVUdubi5ycnI0XQaVgp6eHnR0dMq9HAYZIiKSPSEEkpKSkJqaqulSqAwsLS1hZ2dXrvu8McgQEZHs5YcYGxsbGBsb8waoWk4IgWfPniElJQUAYG9vr/KyGGSIiEjWcnNzpRBjbW2t6XKolIyMjAAAKSkpsLGxUfk0Ey/2JSIiWcu/JsbY2FjDlVBZ5e+z8lzXxCBDRESVAk8nyY869hmDDBEREckWgwwRERGV2vHjx6FQKLRmhBgv9iUiokorZNflN7au0P5eZZ4nKCgIGzZsQGhoKD7++GOpfc+ePejXrx+EEOossVLiERkiIiINMjQ0xLfffovHjx+rbZnZ2dlqW5a2Y5AhIiLSIF9fX9jZ2SE0NLTIPj/99BMaNWoEAwMDODs7Y/HixUrTnZ2dMW/ePAQEBMDc3BxjxoxBREQELC0tsX//fjRo0ADGxsYYMGAAnj17hg0bNsDZ2RnVq1fHhx9+iNzcXGlZmzZtQsuWLWFmZgY7OzsMHTpUut+LNmKQISIi0iAdHR188803WLlyJe7du1dgekxMDAYOHIjBgwfj8uXL+OKLLzB79mxEREQo9Vu0aBGaNGmC8+fPY/bs2QCAZ8+eYcWKFdi6dSsOHTqE48ePo1+/fjhw4AAOHDiATZs2Yd26ddi5c6e0nJycHMybNw8XL17Enj17cPv2bQQFBVXkR1AuvEaGiAilu5ZClWsgiEqjX79+aNq0KebMmYOwsDClaUuWLEGXLl2kcOLm5oZr165h4cKFSgGjc+fOmD59uvT+5MmTyMnJwZo1a+Dq6goAGDBgADZt2oTk5GSYmpqiYcOGePvtt3Hs2DEMGjQIADBixAhpGXXr1sWKFSvw1ltvISMjA6amphX1EaiMR2SIiIi0wLfffosNGzbg+vXrSu3Xr19H27Ztldratm2LGzduKJ0SatmyZYFlGhsbSyEGAGxtbeHs7KwUSGxtbZVOHcXExKBXr16oU6cOzMzM0LFjRwBAfHx8+TawgjDIEBERaYEOHTrAz88PISEhKs1vYmJSoE1PT0/pvUKhKLQtLy8PAPD06VP4+fnB3NwcP/zwA86dO4fdu3cD0N4LiHlqiYiISEvMnz8fTZs2RYMGDaQ2Dw8PnDp1SqnfqVOn4ObmpvLziYry999/4+HDh5g/fz5q164NAIiOjlbrOtSNR2SIiIi0hJeXF4YNG4YVK1ZIbdOnT0dkZCTmzZuHf/75Bxs2bMB3332HGTNmqH39derUgb6+PlauXIl///0X+/btw7x589S+HnVikCEiItIic+fOlU71AEDz5s2xfft2bN26FZ6envj8888xd+7cChlJVLNmTURERGDHjh1o2LAh5s+fj0WLFql9PeqkEJX8toHp6emwsLBAWloazM3NNV0OEWkpjlqSr8zMTNy6dQsuLi4wNDTUdDlUBsXtu9J+f/OIDBEREckWgwwRERHJlkaDzBdffAGFQqH0cnd3l6ZnZmZiwoQJsLa2hqmpKfz9/ZGcnKzBiomIiEibaPyITKNGjZCYmCi9/vjjD2na1KlT8fPPP2PHjh04ceIEEhIS0L9/fw1WS0RERNpE4/eR0dXVhZ2dXYH2tLQ0hIWF4ccff0Tnzp0BAOHh4fDw8MCZM2fQunXrN10qERERaRmNH5G5ceMGHBwcULduXQwbNky6BXJMTAxycnLg6+sr9XV3d0edOnUQFRVV5PKysrKQnp6u9CIiIqLKSaNBxtvbGxERETh06BDWrFmDW7duoX379njy5AmSkpKgr68PS0tLpXlsbW2RlJRU5DJDQ0NhYWEhvfLvTEhERESVj0ZPLXXr1k36d+PGjeHt7Q0nJyds374dRkZGKi0zJCQE06ZNk96np6czzBAREVVSGj+19CpLS0u4ubkhLi4OdnZ2yM7ORmpqqlKf5OTkQq+pyWdgYABzc3OlFxEREVVOWhVkMjIycPPmTdjb26NFixbQ09NDZGSkND02Nhbx8fHw8fHRYJVEREQEAM7Ozli2bJlGa9DoqaUZM2agV69ecHJyQkJCAubMmQMdHR0MGTIEFhYWGDlyJKZNmwYrKyuYm5tj0qRJ8PHx4YglIiIqnZ8nv7l19Vqu8qxRUVFo164dunbtil9++UWNRVV+Gg0y9+7dw5AhQ/Dw4UPUrFkT7dq1w5kzZ1CzZk0AwNKlS1GtWjX4+/sjKysLfn5+WL16tSZLJiIiUruwsDBMmjQJYWFhSEhIgIODg6ZLkg2NnlraunUrEhISkJWVhXv37mHr1q1wdXWVphsaGmLVqlV49OgRnj59il27dhV7fQwREZHcZGRkYNu2bRg3bhx69OiBiIgIadrx48ehUCgQGRmJli1bwtjYGG3atEFsbKzSMtasWQNXV1fo6+ujQYMG2LRpk9J0hUKBdevWoWfPnjA2NoaHhweioqIQFxeHTp06wcTEBG3atMHNmzeleW7evIk+ffrA1tYWpqameOutt3DkyJEit2PEiBHo2bOnUltOTg5sbGwQFhZWjk+oeFp1jQwREVFVs337dri7u6NBgwYYPnw41q9fDyGEUp9PP/0UixcvRnR0NHR1dTFixAhp2u7duzF58mRMnz4dV65cwdixYxEcHIxjx44pLWPevHkICAjAhQsX4O7ujqFDh2Ls2LEICQlBdHQ0hBCYOHGi1D8jIwPdu3dHZGQkzp8/j65du6JXr17S/d5eN2rUKBw6dAiJiYlS2/79+/Hs2TMMGjRIHR9VoRhkiIiINCgsLAzDhw8HAHTt2hVpaWk4ceKEUp+vv/4aHTt2RMOGDfHxxx/j9OnTyMzMBAAsWrQIQUFBGD9+PNzc3DBt2jT0798fixYtUlpGcHAwBg4cCDc3N8yaNQu3b9/GsGHD4OfnBw8PD0yePBnHjx+X+jdp0gRjx46Fp6cn6tevj3nz5sHV1RX79u0rdDvatGlT4GhQeHg43nvvPZiamqrjoyoUgwwREZGGxMbG4uzZsxgyZAiAl4/tGTRoUIFTMY0bN5b+bW9vDwBISUkBAFy/fh1t27ZV6t+2bVtcv369yGXY2toCALy8vJTaMjMzpTviZ2RkYMaMGfDw8IClpSVMTU1x/fr1Io/IAC+PyoSHhwN4ebuUgwcPKh09qggaf9YSERFRVRUWFoYXL14oXdwrhICBgQG+++47qU1PT0/6t0KhAADk5eWVaV2FLaO45c6YMQOHDx/GokWLUK9ePRgZGWHAgAHIzs4uch0BAQH4+OOPERUVhdOnT8PFxQXt27cvU51lxSBDRESkAS9evMDGjRuxePFivPvuu0rT+vbtiy1btsDd3b3E5Xh4eODUqVMIDAyU2k6dOoWGDRuWq75Tp04hKCgI/fr1A/DyCM3t27eLncfa2hp9+/ZFeHg4oqKiEBwcXK4aSoNBhoiISAP279+Px48fY+TIkbCwsFCa5u/vj7CwMCxcuLDE5cycORMDBw5Es2bN4Ovri59//hm7du0qdoRRadSvXx+7du1Cr169oFAoMHv27FIdBRo1ahR69uyJ3NxcpXBVUXiNDBERkQaEhYXB19e3QIgBXgaZ6OhoXLp0qcTl9O3bF8uXL8eiRYvQqFEjrFu3DuHh4ejUqVO56luyZAmqV6+ONm3aoFevXvDz80Pz5s1LnM/X1xf29vbw8/N7I/fDUYjXx3hVMunp6bCwsEBaWhqfu0RERQrZdbnEPqH9vUrsQ29eZmYmbt26BRcXFxgaGmq6nCovIyMDtWrVQnh4OPr3719s3+L2XWm/v3lqiYiIiMotLy8PDx48wOLFi2FpaYnevXu/kfUyyBAREVG5xcfHw8XFBY6OjoiIiICu7puJGAwyREREVG7Ozs4F7kj8JvBiXyIiIpItBhkiIqoUKvnYlUpJHfuMQYaIiGQt/+60z54903AlVFb5++zVOwyXFa+RISIiWdPR0YGlpaX07CFjY2PpdvuknYQQePbsGVJSUmBpaQkdHR2Vl8UgQ0REsmdnZwfg/z9IkeTB0tJS2neqYpAhIiLZUygUsLe3h42NDXJycjRdDpWCnp5euY7E5GOQISKiSkNHR0ctX44kH7zYl4iIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhkS1fTBRARVSYhuy6X2Ce0v9cbqISoauARGSIiIpItBhkiIiKSLQYZIiIiki0GGSIiIpItBhkiIiKSLQYZIiIiki0OvyYiohJxWDlpKx6RISIiItlikCEiIiLZ0pogM3/+fCgUCkyZMkVqy8zMxIQJE2BtbQ1TU1P4+/sjOTlZc0USERGRVtGKIHPu3DmsW7cOjRs3VmqfOnUqfv75Z+zYsQMnTpxAQkIC+vfvr6EqiYiISNtoPMhkZGRg2LBh+P7771G9enWpPS0tDWFhYViyZAk6d+6MFi1aIDw8HKdPn8aZM2c0WDERERFpC40HmQkTJqBHjx7w9fVVao+JiUFOTo5Su7u7O+rUqYOoqKgil5eVlYX09HSlFxEREVVOGh1+vXXrVvz11184d+5cgWlJSUnQ19eHpaWlUrutrS2SkpKKXGZoaCi+/PJLdZdKREREWkhjR2Tu3r2LyZMn44cffoChoaHalhsSEoK0tDTpdffuXbUtm4iIiLSLxoJMTEwMUlJS0Lx5c+jq6kJXVxcnTpzAihUroKurC1tbW2RnZyM1NVVpvuTkZNjZ2RW5XAMDA5ibmyu9iIiIqHLS2KmlLl264PJl5TtFBgcHw93dHbNmzULt2rWhp6eHyMhI+Pv7AwBiY2MRHx8PHx8fTZRMREREWkZjQcbMzAyenp5KbSYmJrC2tpbaR44ciWnTpsHKygrm5uaYNGkSfHx80Lp1a02UTERERFpGq5+1tHTpUlSrVg3+/v7IysqCn58fVq9eremyiIiISEtoVZA5fvy40ntDQ0OsWrUKq1at0kxBREREpNU0fh8ZIiIiIlUxyBAREZFsMcgQERGRbDHIEBERkWwxyBAREZFsMcgQERGRbGnV8GsiqhxCdl0usU9of683UIl24udDpD48IkNERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLKlq+kCiKqykF2XS+wT2t/rDVRCRCRPPCJDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFUUtEVOmVZnSYHHHUGxGPyBAREZGMMcgQERGRbDHIEBERkWwxyBAREZFsMcgQERGRbKkUZP79919110FERERUZioFmXr16uHtt9/G5s2bkZmZqe6aiIiIiEpFpSDz119/oXHjxpg2bRrs7OwwduxYnD17Vt21ERERERVLpSDTtGlTLF++HAkJCVi/fj0SExPRrl07eHp6YsmSJfjvv//UXScRERFRAeW62FdXVxf9+/fHjh078O233yIuLg4zZsxA7dq1ERAQgMTERHXVSURERFRAuYJMdHQ0xo8fD3t7eyxZsgQzZszAzZs3cfjwYSQkJKBPnz7qqpOIiIioAJWetbRkyRKEh4cjNjYW3bt3x8aNG9G9e3dUq/YyF7m4uCAiIgLOzs7qrJWIiIhIiUpBZs2aNRgxYgSCgoJgb29faB8bGxuEhYWVqzgiIiKi4qgUZG7cuFFiH319fQQGBqqyeCKqAtT15Oaq/GRrIlLxGpnw8HDs2LGjQPuOHTuwYcOGchdFREREVBoqBZnQ0FDUqFGjQLuNjQ2++eabUi9nzZo1aNy4MczNzWFubg4fHx8cPHhQmp6ZmYkJEybA2toapqam8Pf3R3JysiolExERUSWkUpCJj4+Hi4tLgXYnJyfEx8eXejmOjo6YP38+YmJiEB0djc6dO6NPnz64evUqAGDq1Kn4+eefsWPHDpw4cQIJCQno37+/KiUTERFRJaTSNTI2Nja4dOlSgVFJFy9ehLW1damX06tXL6X3X3/9NdasWYMzZ87A0dERYWFh+PHHH9G5c2cAL09peXh44MyZM2jdurUqpRMREVElotIRmSFDhuDDDz/EsWPHkJubi9zcXBw9ehSTJ0/G4MGDVSokNzcXW7duxdOnT+Hj44OYmBjk5OTA19dX6uPu7o46deogKipKpXUQERFR5aLSEZl58+bh9u3b6NKlC3R1Xy4iLy8PAQEBZbpGBgAuX74MHx8fZGZmwtTUFLt370bDhg1x4cIF6Ovrw9LSUqm/ra0tkpKSilxeVlYWsrKypPfp6ellqoeIiIjkQ6Ugo6+vj23btmHevHm4ePEijIyM4OXlBScnpzIvq0GDBrhw4QLS0tKwc+dOBAYG4sSJE6qUBeDlhchffvmlyvMTkfbgEGQiKolKQSafm5sb3NzcylWAvr4+6tWrBwBo0aIFzp07h+XLl2PQoEHIzs5Gamqq0lGZ5ORk2NnZFbm8kJAQTJs2TXqfnp6O2rVrl6tGIiIi0k4qBZnc3FxEREQgMjISKSkpyMvLU5p+9OhRlQvKy8tDVlYWWrRoAT09PURGRsLf3x8AEBsbi/j4ePj4+BQ5v4GBAQwMDFRePxEREcmHSkFm8uTJiIiIQI8ePeDp6QmFQqHSykNCQtCtWzfUqVMHT548wY8//ojjx4/j119/hYWFBUaOHIlp06bBysoK5ubmmDRpEnx8fDhiiYiIiACoGGS2bt2K7du3o3v37uVaeUpKCgICApCYmAgLCws0btwYv/76K9555x0AwNKlS1GtWjX4+/sjKysLfn5+WL16dbnWSURERJWHyhf75l/XUh4lPVTS0NAQq1atwqpVq8q9LiIiIqp8VAoy06dPx/Lly/Hdd9+pfFqJiOSJI4mISJuoFGT++OMPHDt2DAcPHkSjRo2gp6enNH3Xrl1qKY6IiIioOCoFGUtLS/Tr10/dtRARERGViUpBJjw8XN11EBEREZWZSs9aAoAXL17gyJEjWLduHZ48eQIASEhIQEZGhtqKIyIiIiqOSkdk7ty5g65duyI+Ph5ZWVl45513YGZmhm+//RZZWVlYu3atuuskIiIiKkClIzKTJ09Gy5Yt8fjxYxgZGUnt/fr1Q2RkpNqKIyIiIiqOSkdkTp48idOnT0NfX1+p3dnZGffv31dLYUREREQlUemITF5eHnJzcwu037t3D2ZmZuUuioiIiKg0VAoy7777LpYtWya9VygUyMjIwJw5c8r92AIiIiKi0lLp1NLixYvh5+eHhg0bIjMzE0OHDsWNGzdQo0YNbNmyRd01EhERERVKpSDj6OiIixcvYuvWrbh06RIyMjIwcuRIDBs2TOniXyIiIqKKpFKQAQBdXV0MHz5cnbUQERERlYlKQWbjxo3FTg8ICFCpGCIiIqKyUCnITJ48Wel9Tk4Onj17Bn19fRgbGzPIEFGlxCd/E2kflUYtPX78WOmVkZGB2NhYtGvXjhf7EhER0Ruj8rOWXle/fn3Mnz+/wNEaIiIiooqitiADvLwAOCEhQZ2LJCIiIiqSStfI7Nu3T+m9EAKJiYn47rvv0LZtW7UURkRERFQSlYJM3759ld4rFArUrFkTnTt3xuLFi9VRFxEREVGJVAoyeXl56q6DiIiIqMxUviEeERFVDuoaVq6u5YT291LLcqhqUCnITJs2rdR9lyxZosoqiIiIiEqkUpA5f/48zp8/j5ycHDRo0AAA8M8//0BHRwfNmzeX+ikUCvVUSURERFQIlYJMr169YGZmhg0bNqB69eoAXt4kLzg4GO3bt8f06dPVWiQRERFRYVS6j8zixYsRGhoqhRgAqF69Or766iuOWiIiIqI3RqUgk56ejv/++69A+3///YcnT56UuygiIiKi0lApyPTr1w/BwcHYtWsX7t27h3v37uGnn37CyJEj0b9/f3XXSERERFQola6RWbt2LWbMmIGhQ4ciJyfn5YJ0dTFy5EgsXLhQrQUSERERFUWlIGNsbIzVq1dj4cKFuHnzJgDA1dUVJiYmai2OiIiIqDjlemhkYmIiEhMTUb9+fZiYmEAIoa66iIiIiEqkUpB5+PAhunTpAjc3N3Tv3h2JiYkAgJEjR3LoNREREb0xKgWZqVOnQk9PD/Hx8TA2NpbaBw0ahEOHDqmtOCIiIqLiqHSNzG+//YZff/0Vjo6OSu3169fHnTt31FIYERERUUlUOiLz9OlTpSMx+R49egQDA4NyF0VERERUGioFmfbt22Pjxo3Se4VCgby8PCxYsABvv/222oojIiIiKo5Kp5YWLFiALl26IDo6GtnZ2fjoo49w9epVPHr0CKdOnVJ3jURERESFUumIjKenJ/755x+0a9cOffr0wdOnT9G/f3+cP38erq6u6q6RiIiIqFBlPiKTk5ODrl27Yu3atfj0008roiYiIiKiUinzERk9PT1cunSpImohIiIiKhOVTi0NHz4cYWFh6q6FiIiIqExUutj3xYsXWL9+PY4cOYIWLVoUeMbSkiVL1FIckbqF7LpcYp/Q/l5voBIiIlKHMgWZf//9F87Ozrhy5QqaN28OAPjnn3+U+igUCvVVR0RERFSMMgWZ+vXrIzExEceOHQPw8pEEK1asgK2tbYUUR0RERFScMl0j8/rTrQ8ePIinT5+qtSAiIiKi0lLpYt98rwcbIiIiojepTEFGoVAUuAaG18QQERGRppTpGhkhBIKCgqQHQ2ZmZuKDDz4oMGpp165d6quQiIiIqAhlCjKBgYFK74cPH67WYoiooDc5ZLw06yIi0iZlCjLh4eEVVQcRERFRmZXrYl8iIiIiTWKQISIiItlikCEiIiLZYpAhIiIi2WKQISIiItlS6enXROpUWZ9IzaHMpA34c0iVHY/IEBERkWxpNMiEhobirbfegpmZGWxsbNC3b1/ExsYq9cnMzMSECRNgbW0NU1NT+Pv7Izk5WUMVExERkTbRaJA5ceIEJkyYgDNnzuDw4cPIycnBu+++q/RE7alTp+Lnn3/Gjh07cOLECSQkJKB///4arJqIiIi0hUavkTl06JDS+4iICNjY2CAmJgYdOnRAWloawsLC8OOPP6Jz584AXt5d2MPDA2fOnEHr1q01UTYRERFpCa26RiYtLQ0AYGVlBQCIiYlBTk4OfH19pT7u7u6oU6cOoqKiCl1GVlYW0tPTlV5ERERUOWlNkMnLy8OUKVPQtm1beHp6AgCSkpKgr68PS0tLpb62trZISkoqdDmhoaGwsLCQXrVr167o0omIiEhDtCbITJgwAVeuXMHWrVvLtZyQkBCkpaVJr7t376qpQiIiItI2WnEfmYkTJ2L//v34/fff4ejoKLXb2dkhOzsbqampSkdlkpOTYWdnV+iyDAwMYGBgUNElExERkRbQ6BEZIQQmTpyI3bt34+jRo3BxcVGa3qJFC+jp6SEyMlJqi42NRXx8PHx8fN50uURERKRlNHpEZsKECfjxxx+xd+9emJmZSde9WFhYwMjICBYWFhg5ciSmTZsGKysrmJubY9KkSfDx8eGIJSIiItJskFmzZg0AoFOnTkrt4eHhCAoKAgAsXboU1apVg7+/P7KysuDn54fVq1e/4UqJiIhIG2k0yAghSuxjaGiIVatWYdWqVW+gIiIiIpITrbjYl4jKhw8GJKKqSmuGXxMRERGVFYMMERERyRaDDBEREckWgwwRERHJFoMMERERyRaDDBEREckWh18TEVGVVZpbF4T293oDlZCqeESGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki8OviVTAp00TEWkHHpEhIiIi2WKQISIiItlikCEiIiLZYpAhIiIi2WKQISIiItniqCUqFB+kRkREcsAjMkRERCRbDDJEREQkWwwyREREJFsMMkRERCRbDDJEREQkWwwyREREJFscfk0qq6xDtPlASCLNUtffFv4uVw08IkNERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLF4ddUoTj8kYjopcp6ywpN4xEZIiIiki0GGSIiIpItBhkiIiKSLQYZIiIiki0GGSIiIpItBhkiIiKSLQ6/JlngMG4iIioMj8gQERGRbDHIEBERkWwxyBAREZFsMcgQERGRbDHIEBERkWxx1BIREVExOGpSu/GIDBEREckWgwwRERHJFoMMERERyRaDDBEREckWgwwRERHJFoMMERERyRaDDBEREckWgwwRERHJFoMMERERyZZGg8zvv/+OXr16wcHBAQqFAnv27FGaLoTA559/Dnt7exgZGcHX1xc3btzQTLFERESkdTQaZJ4+fYomTZpg1apVhU5fsGABVqxYgbVr1+LPP/+EiYkJ/Pz8kJmZ+YYrJSIiIm2k0WctdevWDd26dSt0mhACy5Ytw2effYY+ffoAADZu3AhbW1vs2bMHgwcPfpOlEhERkRbS2mtkbt26haSkJPj6+kptFhYW8Pb2RlRUVJHzZWVlIT09XelFRERElZPWPv06KSkJAGBra6vUbmtrK00rTGhoKL788ssKrU3u+CRXIiKqLLT2iIyqQkJCkJaWJr3u3r2r6ZKIiIiogmhtkLGzswMAJCcnK7UnJydL0wpjYGAAc3NzpRcRERFVTlobZFxcXGBnZ4fIyEipLT09HX/++Sd8fHw0WBkRERFpC41eI5ORkYG4uDjp/a1bt3DhwgVYWVmhTp06mDJlCr766ivUr18fLi4umD17NhwcHNC3b1/NFU1ERERaQ6NBJjo6Gm+//bb0ftq0aQCAwMBARERE4KOPPsLTp08xZswYpKamol27djh06BAMDQ01VTIRERFpEY0GmU6dOkEIUeR0hUKBuXPnYu7cuW+wKiIiIpILrb1GhoiIiKgkDDJEREQkWwwyREREJFsMMkRERCRbDDJEREQkWwwyREREJFsMMkRERCRbWvv066qktE+jDu3vpbZlERGR9inN3/DSfBdUJTwiQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxVFLREQkOxyhSfl4RIaIiIhki0GGiIiIZItBhoiIiGSLQYaIiIhki0GGiIiIZItBhoiIiGSLw68rGIcIElWsvvcWlGv+PY4fqamSwpVUX0WvnyqfN/lgSTk8xJJHZIiIiEi2GGSIiIhIthhkiIiISLYYZIiIiEi2GGSIiIhIthhkiIiISLY4/JqIKhSHH1du3L/aSQ7DptWFR2SIiIhIthhkiIiISLYYZIiIiEi2GGSIiIhIthhkiIiISLYYZIiIiEi2OPxaRvgk7cqnvE9uLg1NP92ZiLRTZflO4REZIiIiki0GGSIiIpItBhkiIiKSLQYZIiIiki0GGSIiIpItjloqh8pyxXdF0vQD5Sp6/ZrevtKQQ41yVt5RW9w/lR/3ccXiERkiIiKSLQYZIiIiki0GGSIiIpItBhkiIiKSLQYZIiIiki0GGSIiIpItDr+mcqnoBwZy2CJp+qGUml5/ScpbX0m/QxU9vLwkFV2fOpav6b9DVf3vJI/IEBERkWwxyBAREZFsMcgQERGRbDHIEBERkWwxyBAREZFsMcgQERGRbHH4tZbj05uLp+1PHtb2obvagJ9R8ar65yOH7ZdDjZUZj8gQERGRbMkiyKxatQrOzs4wNDSEt7c3zp49q+mSiIiISAtofZDZtm0bpk2bhjlz5uCvv/5CkyZN4Ofnh5SUFE2XRkRERBqm9UFmyZIlGD16NIKDg9GwYUOsXbsWxsbGWL9+vaZLIyIiIg3T6iCTnZ2NmJgY+Pr6Sm3VqlWDr68voqKiNFgZERERaQOtHrX04MED5ObmwtbWVqnd1tYWf//9d6HzZGVlISsrS3qflpYGAEhPT1d7fVnPMtS+zNc9zcyu0BrKu/yS5i9JRS+/vLS9PnWo6J8hbVcV9nFxuP2a335N/x0vr4r4fn11uUKI4jsKLXb//n0BQJw+fVqpfebMmaJVq1aFzjNnzhwBgC+++OKLL774qgSvu3fvFpsVtPqITI0aNaCjo4Pk5GSl9uTkZNjZ2RU6T0hICKZNmya9z8vLw6NHj2BtbQ2FQiG1p6eno3bt2rh79y7Mzc0rZgM0qLJvH8BtrCy4jZVDZd/Gyr59gPZtoxACT548gYODQ7H9tDrI6Ovro0WLFoiMjETfvn0BvAwmkZGRmDhxYqHzGBgYwMDAQKnN0tKyyHWYm5trxQ6rKJV9+wBuY2XBbawcKvs2VvbtA7RrGy0sLErso9VBBgCmTZuGwMBAtGzZEq1atcKyZcvw9OlTBAcHa7o0IiIi0jCtDzKDBg3Cf//9h88//xxJSUlo2rQpDh06VOACYCIiIqp6tD7IAMDEiROLPJWkKgMDA8yZM6fAaajKorJvH8BtrCy4jZVDZd/Gyr59gHy3USFESeOaiIiIiLSTVt8Qj4iIiKg4DDJEREQkWwwyREREJFsMMkRERCRbVTLIrFq1Cs7OzjA0NIS3tzfOnj2r6ZIAAKGhoXjrrbdgZmYGGxsb9O3bF7GxsUp9OnXqBIVCofT64IMPlPrEx8ejR48eMDY2ho2NDWbOnIkXL14o9Tl+/DiaN28OAwMD1KtXDxEREQXqUffn9MUXXxSo3d3dXZqemZmJCRMmwNraGqampvD39y9wV2dt3bZ8zs7OBbZRoVBgwoQJAOS5/37//Xf06tULDg4OUCgU2LNnj9J0IQQ+//xz2Nvbw8jICL6+vrhx44ZSn0ePHmHYsGEwNzeHpaUlRo4ciYwM5ee/XLp0Ce3bt4ehoSFq166NBQsWFKhlx44dcHd3h6GhIby8vHDgwIEy11LWbczJycGsWbPg5eUFExMTODg4ICAgAAkJCUrLKGzfz58/XxbbCABBQUEF6u/atatSHznvRwCF/m4qFAosXLhQ6qPN+7E03xHa9He0NLWoRbkfiCQzW7duFfr6+mL9+vXi6tWrYvTo0cLS0lIkJydrujTh5+cnwsPDxZUrV8SFCxdE9+7dRZ06dURGRobUp2PHjmL06NEiMTFReqWlpUnTX7x4ITw9PYWvr684f/68OHDggKhRo4YICQmR+vz777/C2NhYTJs2TVy7dk2sXLlS6OjoiEOHDkl9KuJzmjNnjmjUqJFS7f/99580/YMPPhC1a9cWkZGRIjo6WrRu3Vq0adNGFtuWLyUlRWn7Dh8+LACIY8eOCSHkuf8OHDggPv30U7Fr1y4BQOzevVtp+vz584WFhYXYs2ePuHjxoujdu7dwcXERz58/l/p07dpVNGnSRJw5c0acPHlS1KtXTwwZMkSanpaWJmxtbcWwYcPElStXxJYtW4SRkZFYt26d1OfUqVNCR0dHLFiwQFy7dk189tlnQk9PT1y+fLlMtZR1G1NTU4Wvr6/Ytm2b+Pvvv0VUVJRo1aqVaNGihdIynJycxNy5c5X27au/u9q8jUIIERgYKLp27apU/6NHj5T6yHk/CiGUti0xMVGsX79eKBQKcfPmTamPNu/H0nxHaNPf0ZJqUZcqF2RatWolJkyYIL3Pzc0VDg4OIjQ0VINVFS4lJUUAECdOnJDaOnbsKCZPnlzkPAcOHBDVqlUTSUlJUtuaNWuEubm5yMrKEkII8dFHH4lGjRopzTdo0CDh5+cnva+Iz2nOnDmiSZMmhU5LTU0Venp6YseOHVLb9evXBQARFRWl9dtWlMmTJwtXV1eRl5cnhJD3/hNCFPhyyMvLE3Z2dmLhwoVSW2pqqjAwMBBbtmwRQghx7do1AUCcO3dO6nPw4EGhUCjE/fv3hRBCrF69WlSvXl3aRiGEmDVrlmjQoIH0fuDAgaJHjx5K9Xh7e4uxY8eWuhZVtrEwZ8+eFQDEnTt3pDYnJyexdOnSIufR9m0MDAwUffr0KXKeyrgf+/TpIzp37qzUJqf9+Pp3hDb9HS1NLepSpU4tZWdnIyYmBr6+vlJbtWrV4Ovri6ioKA1WVri0tDQAgJWVlVL7Dz/8gBo1asDT0xMhISF49uyZNC0qKgpeXl5Kdz728/NDeno6rl69KvV59TPI75P/GVTk53Tjxg04ODigbt26GDZsGOLj4wEAMTExyMnJUVqnu7s76tSpI61T27ftddnZ2di8eTNGjBih9MBSOe+/1926dQtJSUlK67KwsIC3t7fSfrO0tETLli2lPr6+vqhWrRr+/PNPqU+HDh2gr6+vtE2xsbF4/Phxqba7NLWoS1paGhQKRYHnuM2fPx/W1tZo1qwZFi5cqHS4Xg7bePz4cdjY2KBBgwYYN24cHj58qFR/ZdqPycnJ+OWXXzBy5MgC0+SyH1//jtCmv6OlqUVdZHFnX3V58OABcnNzCzzewNbWFn///beGqipcXl4epkyZgrZt28LT01NqHzp0KJycnODg4IBLly5h1qxZiI2Nxa5duwAASUlJhW5f/rTi+qSnp+P58+d4/PhxhXxO3t7eiIiIQIMGDZCYmIgvv/wS7du3x5UrV5CUlAR9ff0CXwy2trYl1q0N21aYPXv2IDU1FUFBQVKbnPdfYfJrKmxdr9ZrY2OjNF1XVxdWVlZKfVxcXAosI39a9erVi9zuV5dRUi3qkJmZiVmzZmHIkCFKD9b78MMP0bx5c1hZWeH06dMICQlBYmIilixZIott7Nq1K/r37w8XFxfcvHkTn3zyCbp164aoqCjo6OhUuv24YcMGmJmZoX///krtctmPhX1HaNPf0dLUoi5VKsjIyYQJE3DlyhX88ccfSu1jxoyR/u3l5QV7e3t06dIFN2/ehKur65sus0y6desm/btx48bw9vaGk5MTtm/fDiMjIw1WVjHCwsLQrVs3pUfQy3n/0csLfwcOHAghBNasWaM0bdq0adK/GzduDH19fYwdOxahoaGyuOX74MGDpX97eXmhcePGcHV1xfHjx9GlSxcNVlYx1q9fj2HDhsHQ0FCpXS77sajviKqoSp1aqlGjBnR0dApcNZ2cnAw7OzsNVVXQxIkTsX//fhw7dgyOjo7F9vX29gYAxMXFAQDs7OwK3b78acX1MTc3h5GR0Rv7nCwtLeHm5oa4uDjY2dkhOzsbqampRa5TTtt2584dHDlyBKNGjSq2n5z336s1FbcuOzs7pKSkKE1/8eIFHj16pJZ9++r0kmopj/wQc+fOHRw+fFjpaExhvL298eLFC9y+fbvY+l+tXdPb+Kq6deuiRo0aSj+blWE/AsDJkycRGxtb4u8noJ37sajvCG36O1qaWtSlSgUZfX19tGjRApGRkVJbXl4eIiMj4ePjo8HKXhJCYOLEidi9ezeOHj1a4PBlYS5cuAAAsLe3BwD4+Pjg8uXLSn9w8v/oNmzYUOrz6meQ3yf/M3hTn1NGRgZu3rwJe3t7tGjRAnp6ekrrjI2NRXx8vLROOW1beHg4bGxs0KNHj2L7yXn/AYCLiwvs7OyU1pWeno4///xTab+lpqYiJiZG6nP06FHk5eVJQc7Hxwe///47cnJylLapQYMGqF69eqm2uzS1qCo/xNy4cQNHjhyBtbV1ifNcuHAB1apVk07HaPs2vu7evXt4+PCh0s+m3PdjvrCwMLRo0QJNmjQpsa827ceSviO06e9oaWpRG7VeOiwDW7duFQYGBiIiIkJcu3ZNjBkzRlhaWipdwa0p48aNExYWFuL48eNKQ/+ePXsmhBAiLi5OzJ07V0RHR4tbt26JvXv3irp164oOHTpIy8gfWvfuu++KCxcuiEOHDomaNWsWOrRu5syZ4vr162LVqlWFDq1T9+c0ffp0cfz4cXHr1i1x6tQp4evrK2rUqCFSUlKEEC+H6tWpU0ccPXpUREdHCx8fH+Hj4yOLbXtVbm6uqFOnjpg1a5ZSu1z335MnT8T58+fF+fPnBQCxZMkScf78eWnEzvz584WlpaXYu3evuHTpkujTp0+hw6+bNWsm/vzzT/HHH3+I+vXrKw3bTU1NFba2tuL9998XV65cEVu3bhXGxsYFhrTq6uqKRYsWievXr4s5c+YUOqS1pFrKuo3Z2dmid+/ewtHRUVy4cEHpdzN/lMfp06fF0qVLxYULF8TNmzfF5s2bRc2aNUVAQIAstvHJkydixowZIioqSty6dUscOXJENG/eXNSvX19kZmZWiv2YLy0tTRgbG4s1a9YUmF/b92NJ3xFCaNff0ZJqUZcqF2SEEGLlypWiTp06Ql9fX7Rq1UqcOXNG0yUJIV4OFyzsFR4eLoQQIj4+XnTo0EFYWVkJAwMDUa9ePTFz5kyl+5AIIcTt27dFt27dhJGRkahRo4aYPn26yMnJUepz7Ngx0bRpU6Gvry/q1q0rreNV6v6cBg0aJOzt7YW+vr6oVauWGDRokIiLi5OmP3/+XIwfP15Ur15dGBsbi379+onExERZbNurfv31VwFAxMbGKrXLdf8dO3as0J/LwMBAIcTLoaSzZ88Wtra2wsDAQHTp0qXAtj98+FAMGTJEmJqaCnNzcxEcHCyePHmi1OfixYuiXbt2wsDAQNSqVUvMnz+/QC3bt28Xbm5uQl9fXzRq1Ej88ssvStNLU0tZt/HWrVtF/m7m3x8oJiZGeHt7CwsLC2FoaCg8PDzEN998oxQCtHkbnz17Jt59911Rs2ZNoaenJ5ycnMTo0aMLBF8578d869atE0ZGRiI1NbXA/Nq+H0v6jhBCu/6OlqYWdVAIIYR6j/EQERERvRlV6hoZIiIiqlwYZIiIiEi2GGSIiIhIthhkiIiISLYYZIiIiEi2GGSIiIhIthhkiIiISLYYZIhIlpydnbFs2TJNl0FEGsYgQ1SFRUVFQUdHp8RnQlVW33//PZo0aQJTU1NYWlqiWbNmCA0N1XRZRFQGupougIg0JywsDJMmTUJYWBgSEhLg4OCg6ZLemPXr12PKlClYsWIFOnbsiKysLFy6dAlXrlypsHVmZ2dDX1+/wpZPVBXxiAxRFZWRkYFt27Zh3Lhx6NGjByIiIpSmHz9+HAqFApGRkWjZsiWMjY3Rpk0bxMbGKvVbs2YNXF1doa+vjwYNGmDTpk1K0xUKBdatW4eePXvC2NgYHh4eiIqKQlxcHDp16gQTExO0adMGN2/elOa5efMm+vTpA1tbW5iamuKtt97CkSNHityWESNGoGfPnkptOTk5sLGxQVhYWKHz7Nu3DwMHDsTIkSNRr149NGrUCEOGDMHXX3+t1G/9+vVo1KgRDAwMYG9vj4kTJ0rT4uPj0adPH5iamsLc3BwDBw5EcnKyNP2LL75A06ZN8X//939wcXGBoaEhACA1NRWjRo1CzZo1YW5ujs6dO+PixYtFbh8RFY1BhqiK2r59O9zd3dGgQQMMHz4c69evR2GPXvv000+xePFiREdHQ1dXFyNGjJCm7d69G5MnT8b06dNx5coVjB07FsHBwTh27JjSMubNm4eAgABcuHAB7u7uGDp0KMaOHYuQkBBER0dDCKEUEDIyMtC9e3dERkbi/Pnz6Nq1K3r16oX4+PhCt2XUqFE4dOgQEhMTpbb9+/fj2bNnGDRoUKHz2NnZ4cyZM7hz506Rn9GaNWswYcIEjBkzBpcvX8a+fftQr149AEBeXh769OmDR48e4cSJEzh8+DD+/fffAuuLi4vDTz/9hF27duHChQsAgPfeew8pKSk4ePAgYmJi0Lx5c3Tp0gWPHj0qshYiKoLaH0NJRLLQpk0bsWzZMiGEEDk5OaJGjRrS05yF+P9PEj5y5IjU9ssvvwgA4vnz59IyRo8erbTc9957T3Tv3l16D0B89tln0vuoqCgBQISFhUltW7ZsEYaGhsXW26hRI7Fy5UrpvZOTk1i6dKn0vmHDhuLbb7+V3vfq1UsEBQUVubyEhATRunVrAUC4ubmJwMBAsW3bNpGbmyv1cXBwEJ9++mmh8//2229CR0dHxMfHS21Xr14VAMTZs2eFEELMmTNH6OnpiZSUFKnPyZMnhbm5eYEnKru6uop169YV+xkQUUE8IkNUBcXGxuLs2bMYMmQIAEBXVxeDBg0q9DRM48aNpX/b29sDAFJSUgAA169fR9u2bZX6t23bFtevXy9yGba2tgAALy8vpbbMzEykp6cDeHlEZsaMGfDw8IClpSVMTU1x/fr1Io/IAC+PyoSHhwMAkpOTcfDgQaWjR6+zt7dHVFQULl++jMmTJ+PFixcIDAxE165dkZeXh5SUFCQkJKBLly6Fzn/9+nXUrl0btWvXltoaNmwIS0tLpe13cnJCzZo1pfcXL15ERkYGrK2tYWpqKr1u3bqldHqNiEqHF/sSVUFhYWF48eKF0sW9QggYGBjgu+++g4WFhdSup6cn/VuhUAB4eVqlLApbRnHLnTFjBg4fPoxFixahXr16MDIywoABA5CdnV3kOgICAvDxxx8jKioKp0+fhouLC9q3b19ibZ6envD09MT48ePxwQcfoH379jhx4gRatmxZpm0siomJidL7jIwM2Nvb4/jx4wX6WlpaqmWdRFUJgwxRFfPixQts3LgRixcvxrvvvqs0rW/fvtiyZQs++OCDUi3Lw8MDp06dQmBgoNR26tQpNGzYsFw1njp1CkFBQejXrx+Al1/+t2/fLnYea2tr9O3bF+Hh4YiKikJwcHCZ15tf99OnT2FmZgZnZ2dERkbi7bffLtDXw8MDd+/exd27d6WjMteuXUNqamqx29+8eXMkJSVBV1cXzs7OZa6RiJQxyBBVMfv378fjx48xcuRIpSMvAODv74+wsLBSB5mZM2di4MCBaNasGXx9ffHzzz9j165dxY4wKo369etj165d6NWrFxQKBWbPnl2qo0CjRo1Cz549kZubqxSuCjNu3Dg4ODigc+fOcHR0RGJiIr766ivUrFkTPj4+AF6OOvrggw9gY2ODbt264cmTJzh16hQmTZoEX19feHl5YdiwYVi2bBlevHiB8ePHo2PHjsUezfH19YWPjw/69u2LBQsWwM3NDQkJCfjll1/Qr18/tR0JIqoqeI0MURUTFhYGX1/fAiEGeBlkoqOjcenSpVItq2/fvli+fDkWLVqERo0aYd26dQgPD0enTp3KVeOSJUtQvXp1tGnTBr169YKfnx+aN29e4ny+vr6wt7eHn59fiffE8fX1xZkzZ/Dee+/Bzc0N/v7+MDQ0RGRkJKytrQEAgYGBWLZsGVavXo1GjRqhZ8+euHHjBoCXp8P27t2L6tWro0OHDvD19UXdunWxbdu2YterUChw4MABdOjQAcHBwXBzc8PgwYNx584d6fohIio9hRCFjLckIpKhjIwM1KpVC+Hh4ejfv7+myyGiN4CnlohI9vLy8vDgwQMsXrwYlpaW6N27t6ZLIqI3hEGGiGQvPj4eLi4ucHR0REREBHR1+aeNqKrgqSUiIiKSLV7sS0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREssUgQ0RERLLFIENERESyxSBDREREsvX/AC5ycxYTQkc4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}